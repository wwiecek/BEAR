
# Benchmarks of Empirical Accuracy in Research

BEAR is a "meta"-database containing z values, effect sizes and standard errors 
from existing databases in various scientific disciplines. 
It doesn't contribute any new data, but 
repackages and merges what's publicly available in a manner which we hope 
is maximally user-friendly. Our intention is to help researchers interested in 
issues of replication, exchangeability, meta-analysis etc. etc.

![](doc/bear_banner.png)

# Datasets included in BEAR

References, details of data processing, and short descriptions of each dataset 
included are in [a separate PDF in the main folder](datasets.pdf). Main data processing script is [process_data.md](process_data.md) and 

Here is a short summary of what's included in BEAR:

```{r echo=FALSE, messages=FALSE}
library(dplyr)
source("R/settings.R")
bear <- readRDS("data/BEAR.rds")
bear %>% 
  group_by(dataset) %>% 
  summarise(n_z = n(), 
            n_meta = n_distinct(metaid), 
            n_study = n_distinct(studyid), 
            mean_k  = n()/n_distinct(studyid), 
            pct_signif = sum(abs(z)>1.96)/n(),
            type = {
              tab <- table(method)
              counts <- c(tab)
              na_count <- sum(is.na(method))
              if (na_count > 0) counts <- c(counts, `NA` = na_count)
              paste(paste0(names(counts), ": ", counts), collapse = ", ")
            }
  ) %>% mutate(dataset = bear_names[dataset])
```


# Modelling datasets using mixture models

## Optional post-processing 

To fit mixture models described in the accompanying paper, we do minimal postprocess (`postprocess.R`):

- Add number of observations in each study and number of studies in each meta-analysis 
- Add study weights as 1 / (N values reported in that study)
- For large datasets (>50,000 rows), choose one z-value per study
- Truncate very large z values (z > 20) and replace "z = 0" statements with "z < 0.5" (z=0 would not work when fitting mixtures)

In `fit_mixtures.R` we create a fit for each of the datasets, saved in `mixtures/`. This can take a few minutes per dataset.

We calculate summaries for each dataset (e.g. probablity of significance, replication, correct sign) in `calculate_psr.R`



