# Sources of data in this project

Up-to-date document with short descriptions is 
[available as a Google Doc](https://docs.google.com/document/d/1ZZAEwfHS0aAELN1w1lqEO6-eeu31_WdL/edit?usp=sharing&ouid=114240127695432531696&rtpof=true&sd=true).

General methodology for creating sets of z-values:

- Remove non-RCTs and various malformed inputs
- Extract or calculate `z`:
    - `z = b/se` in most datasets 
    - if no SE is avaialble, but we have p-value, we do `-qnorm(p/2)` (or `sign(b)*qnorm(1-p/2)` to retrieve sign of z)
    - for Barnett and Wren there are no SE but we have 95% intervals; we switch to log scale, calculate `se` by dividing by 3.96 and `b` as midpoint
    - for Sladekova et al we use Fisher's z transformation of correlation coefficients, `b = 0.5*log((1 + yi)/(1 - yi))`
    - for MetaPsy database it is standardised to Hedges' $g$
- If some `z` values were truncated in some way, keep an indicator for that (Jager and Leek only)
- Extract year
    - if available, year of intervention (Askarov et al); if not, year of study; 
    - in one case (Yang et al) I retrieve year of study it from study titles, since half of them have dates available
- Keep study indicator, labelled `studyid`
- For sets of meta-analyses, also retain `metaid` indicator
- Remove studies where z's are NA before saving (to reduce size of data)
- Join into a single table with column `dataset` 



Post-processing:

- Truncate very large z values ourselves 
- Add number of observations in each study, 
- Add number of studies in each meta-analysis 
- For large datasets, choose one z-value per study
- Add study weights as 1 / (N values reported in that study)


```{r echo=FALSE, messages=FALSE}
library(dplyr)
bear <- readRDS("data/BEAR.rds")
bear %>% 
  group_by(dataset) %>% 
  summarise(n_z = n(), 
            n_meta = n_distinct(metaid), 
            median_j  = median(j),
            n_study = n_distinct(studyid), 
            median_k = median(k), 
            pct_signif = sum(abs(z)>1.96)/n())
```
