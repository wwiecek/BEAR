
# Benchmarks of Empirical Accuracy in Research

BEAR is a "meta"-database containing z values, effect sizes and standard errors 
from existing databases in various scientific disciplines. 
It doesn't contribute any new data, but 
repackages and merges what's publicly available in a manner which we hope 
is maximally user-friendly. Our intention is to help researchers interested in 
issues of replication, exchangeability, meta-analysis etc. etc.

**If you only want to grab data, head to the GitHub Releases page and grab `BEAR.rds` [at this link](https://github.com/wwiecek/BEAR/releases).**

To work with individual datasets or to do more manipulation of inputs, we include a "submodule" repo in `data/`. See below for instructions on how to download them. Our entire workflow is in `main.R`.

![](doc/bear_banner.png)

# Datasets included in BEAR

References, details of data availability and processing, links to raw data, and short descriptions of each dataset are in [a separate PDF in the doc folder](doc/datasets.pdf).

Here is a short summary of what's included in BEAR:

```{r echo=FALSE, messages=FALSE}
library(dplyr)
library(readr)
source("R/settings.R")
bear <- readRDS("BEAR.rds")
tab_psr <- suppressMessages(
  readr::read_csv("paper/power_sign_rep.csv", show_col_types = FALSE))

bear_summary <- bear %>% 
  group_by(dataset) %>% 
  summarise(n_z = n(), 
            n_meta = n_distinct(metaid), 
            n_study = n_distinct(studyid), 
            mean_k  = n()/n_distinct(studyid), 
            pct_signif = sum(abs(z)>1.96)/n()
  ) %>% 
  mutate(domain = bear_domain[dataset])

bear_summary %>% 
  arrange(dataset) %>% 
  relocate(domain, .after = dataset) %>% 
  mutate(dataset = bear_names[dataset]) %>% 
  rename(n_values = n_z)
```

Datasets fall into four main categories that will be useful for different types of metascientific investigations: curated datasets of single studies, curated sets of meta-analyses (i.e. with additional `metaid` grouping column), large-scale scraped datasets from PubMed/Medline, and replication datasets. Additional groupings are available in some datasets.

```{r echo=FALSE, messages=FALSE}
bear_summary %>% 
  mutate(gr = bear_classification[dataset]) %>% 
  # mutate(gr = ifelse(gr == "meta", "curated", gr)) %>% 
  group_by(gr) %>% 
  summarise(n_datasets = n(),
            n_study = sum(n_study),
            n_meta = ifelse(sum(n_meta) == n_datasets, NA, sum(n_meta)),
            n_values = sum(n_z),
            pct_signif = sum(pct_signif*n_z) / sum(n_z))
```



# Downloading BEAR data

If you only want to grab data, head to the GitHub Releases page and grab `BEAR.rds` [at this link](https://github.com/wwiecek/BEAR/releases).

To work with individual datasets or to do more manipulation of inputs, they are included as a "submodule" repo in `data/`. After cloning this repository, you need the following command

```
git submodule update --init --recursive
```

In other words, downloading all of the input data is opt-in rather than part of this repo, to keep the repo size minimal.



# Modelling datasets using mixture models

## Optional post-processing 

To fit mixture models described in the accompanying paper, we do minimal postprocess (`postprocess.R`):

- Add number of observations in each study and number of studies in each meta-analysis 
- Add study weights as 1 / (N values reported in that study)
- For large datasets (>50,000 rows), choose one z-value per study
- Truncate very large z values (z > 20) and replace "z = 0" statements with "z < 0.5" (z=0 would not work when fitting mixtures)

In `fit_mixtures.R` we create a fit for each of the datasets, saved in `mixtures/`. This can take a few minutes per dataset.

We calculate summaries for each dataset (e.g. probablity of significance, replication, correct sign) in `calculate_psr.R`


# Results of mixture modelling

```{r echo=FALSE, messages=FALSE}
tab_psr %>% 
  mutate(gr = bear_classification[dataset]) %>% 
  mutate(gr = ifelse(gr %in% c("curated", "meta"), "curated+meta", gr)) %>% 
  arrange(gr, desc(assurance)) %>% 
  mutate(dataset = bear_names[dataset]) %>% 
  transmute(dataset, omega, PoS=assurance, PoS_80=pos_80pct, replication, sign)

```

`omega` is relative publication probability based on crossing of the |z|=1.96 threshold; `PoS` is probability of significance (assurance) and `PoS_80` is proportion of studies that reach 80% power; `replication` and `sign` probabilities are described in accompanying paper.


