
# Benchmarks of Empirical Accuracy in Research

## Arel-Bundock et al 2022 (political science)

Arel-Bundock, V., Briggs, R.C., Doucouliagos, H., Mendoza Aviña, M. and Stanley, T.D., 2022. Quantitative political science research is greatly underpowered. I4R Discussion Paper Series.

Research question: assess statistical power in political science research

Data collection: Authors searched for (and emailed authors of) meta-analysis articles across 141 journals in political science; this led to "a dataset of 16,649 hypothesis tests, grouped in 351 meta-analyses, reported in 46 peer-reviewed meta-analytic articles"

Notes: There is possibly a considerable overlap with Askarov et al dataset, but these two datasets are worth analysing separately given their different focus.

Data availability: replication package can be found at [dataset from https://osf.io/fgdet]() (_ps_power_replication_20241010.zip_). All data are in public domain. We contacted Ryan Briggs who was not aware of any licensing issues.

Data file: using the same code as original authors (replication package runs on makefiles), but skipping some additional processing that they've done, we write a single `estimates.csv` file (3.3 MB).

Data processing: no additional manipulation of data done when creating BEAR dataset.



## Askarov et al 2023 (economics)

Askarov, Z., Doucouliagos, A., Doucouliagos, H. and Stanley, T.D., 2023. The significance of data-sharing policy. Journal of the European Economic Association, 21(3), pp.1191-1226.

Research question: Impact of mandatory data sharing on (excessive) statistical significance in economics papers

Data collection: 64,000 economic estimates surveyed by Ioannidis et al. (2017) + "searching through numerous databases, journal websites, and also an email survey of 109 authors known to have produced meta-analyses of economic topics. We cast the widest net possible to capture empirical economic estimates. Thus, some of the papers included in these meta-analyses were originally published in political science and psychology journals. Nevertheless, these meta-studies deal with economics topics." Later: "we collect 166,924 parameter estimates from 345 distinct research areas, of which
20,121 were published in 24 of the leading general interest and field journals" The paper mentions 12,521 observations before and 2,537 after data sharing policies in 24 journals; reproducibility dataset has 32 journals (authors removed some smaller ones) and 22,172 rows

Data availability: file `Mandatory data-sharing 30 Aug 2022.dta` may be downloaded from [https://github.com/anthonydouc/Datasharing/blob/master/Stata/](); reproducibility package at [https://dataverse.harvard.edu/file.xhtml?fileId=7884702&version=1.0]()

Data file: same unprocessed `Mandatory data-sharing 30 Aug 2022.dta` file (11.6 MB)

Data processing: no additional manipulation of data done when creating BEAR dataset.



## Barnett and Wren 

Barnett, A.G. and Wren, J.D., 2019. Examination of CIs in health and medical journals from 1976 to 2019: an observational study. BMJ open, 9(11), p.e032506.

A collection of confidence intervals for ratio estimates from Medline papers from 1976-2019

Research question: Bias for statistical significance in health and medical journals

Data collection: authors scraped (regular expression followed by an independent check using another data mining algorithm, manual checks for 10,000 intervals) 968,000 CIs from abstracts and 350,000 from full text

Data availability: The dataset Georgescu.Wren.RData may be downloaded from https://github.com/agbarnett/intervals/tree/master/data and also https://github.com/jdwren/ASEC Availability: no clear licence given in the repo but the article is CC BY 4.0 in BMJ Open. 

Notes: these are “ratio estimates” like odds ratios, hazard ratios and risk ratios. Note that binary outcomes tend to have (much) lower information than continuous outcomes.

Data file: Georgescu.Wren.RData

Data processing: (explain these steps):

```
  # 0.3% of available values have CI widths other than 95%, let's remove these
  # but if ci.level is unknown, assume that it's actually 95% 
  mutate(ci.level = ifelse(is.na(ci.level), 0.95, ci.level)) %>% 
  filter(ci.level == 0.95) %>% 
  # Remove cases where CI is zero
  filter(lower < upper) %>% 
  # To allow for log(lower), add a tiny value to zeroes (~0.1% of the sample)
  mutate(lower = ifelse(lower > 0, lower, 1e-05)) %>%
  mutate(se = (log(upper) - log(lower))/(2*1.96)) %>% 
  mutate(b = (log(upper) + log(lower))/2) %>% 
```

