---
title: "Datasets included in BEAR"
subtitle: "Appendix"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  pdf_document:
    toc: false
    number_sections: false
geometry: margin=1in
fontsize: 11pt
---

## Arel-Bundock et al 2022 (political science)

Reference: Arel-Bundock et al 2022

Research question: assess statistical power in political science research

Data availability: replication package (zip file with date 20241010) can be found at <https://osf.io/fgdet>. All data are in public domain. We notified one of the authors, Ryan Briggs, who was not aware of any licensing issues.

Data description and source: 
Authors searched for (and emailed authors of) meta-analysis articles across 141 journals in political science; this led to "a dataset of 16,649 hypothesis tests, grouped in 351 meta-analyses, reported in 46 peer-reviewed meta-analytic articles"

Notes: There is possibly a considerable overlap with Askarov et al dataset, but these two datasets are worth analysing separately given their different focus.

Data processing: using the same code as original authors (replication package runs on makefiles), but skipping some additional processing that they've done, we write a single `estimates.csv` file (3.3 MB). No manipulation of data was done when creating BEAR dataset.



## Askarov et al 2023 (economics)

Reference: Askarov et al 2023

Research question: 
Impact of mandatory data sharing on (excessive) statistical significance in economics papers

Data availability: 
file `Mandatory data-sharing 30 Aug 2022.dta` may be downloaded from <https://github.com/anthonydouc/Datasharing/blob/master/Stata/>; reproducibility package at <https://dataverse.harvard.edu/file.xhtml?fileId=7884702&version=1.0>

Data description and source: 
The paper includes 64,000 economic estimates surveyed by Ioannidis et al. (2017) plus results of "searching through numerous databases, journal websites, and also an email survey of 109 authors known to have produced meta-analyses of economic topics. We cast the widest net possible to capture empirical economic estimates. Thus, some of the papers included in these meta-analyses were originally published in political science and psychology journals. Nevertheless, these meta-studies deal with economics topics." Later they note: "we collect 166,924 parameter estimates from 345 distinct research areas, of which
20,121 were published in 24 of the leading general interest and field journals" The paper mentions 12,521 observations before and 2,537 after data sharing policies in 24 journals; reproducibility dataset has 32 journals (authors removed some smaller ones) and 22,172 rows.


Data processing: 
We use the same file `Mandatory data-sharing 30 Aug 2022.dta` (11.6 MB) as in the reproducibility package. No additional manipulation of data was done when creating BEAR dataset.



## Barnett and Wren 

Reference: Barnett and Wren 2019

Research question: Bias for statistical significance in health and medical journals

Data availability: 
The dataset Georgescu.Wren.RData may be downloaded from <https://github.com/agbarnett/intervals/tree/master/data> and also <https://github.com/jdwren/ASEC>. Data file is `Georgescu.Wren.RData`. Availability: no clear licence given in the repo but the article is CC BY 4.0 in BMJ Open. 

Data description and source: 
The dataset is a collection of confidence intervals for ratio estimates from Medline papers from 1976-2019.
Authors scraped (via regular expressions, followed by an independent check using another data mining algorithm, manual checks for 10,000 intervals) 968,000 CIs from abstracts and 350,000 from full texts.

Notes: 
These are “ratio estimates” like odds ratios, hazard ratios and risk ratios. Note that binary outcomes tend to have (much) lower information than continuous outcomes.

Data processing: 
We excluded a small fraction of data. First we restricted to 95% confidence intervals only: where `ci.level` was missing we assumed it was 0.95; among intervals with a known `ci.level`, about 0.3% were not 95% and were dropped. We dropped intervals with non-positive width (i.e. `lower >= upper`). We used log scale (we replaced zero or negative lower bounds with a small positive constant; this affected about 0.1% of the sample) and backed out an approximate standard error and point estimate under the usual normal approximation for a 95% CI, setting point estimate to half-point of the interval.



## Bartoš et al. 2025 (exercise)

Reference: Bartoš et al 2025

This paper builds on another analysis:

Reference: Singh et al 2025

Research question: 
What is the effect of physical exercise on cognition, memory, and executive function; with extra focus on selective reporting and heterogeneity.

Data availability: 
Replication data are publicly available via PsyArXiv  <https://osf.io/preprints/psyarxiv/qr8e2_v1> under Creative Commons By Attribution 4.0 license.

Data description and source: 
"2,239 effect-size estimates from 215 meta-analyses of randomized controlled trials" was done by Bartoš et al (2025) based on extension of work by Singh et al (2025)

Data processing: 
z-values were computed as effect size divided by its reported standard error. No filtering or recoding beyond variable renaming was required.

Additional variables used: 



## Brodeur et al. 2024 (economics)

Reference: Brodeur et al 2024

Research question: are preregistration and pre-analysis plans associated with reduced p-hacking and publication bias in economics?

Data collection: manually extracted test statistics from RCTs in leading economics journals published 2018-2021. Extraction was performed table by table. Each row corresponds to a single reported test statistic, linked to a paper-level identifier. Unlike most other sources in BEAR, this means we have dozens of estimates per paper: 314 articles with 16,390 estimates

Data availability: replication data file `merged.dta` may be downloaded from <https://dataverse.harvard.edu/file.xhtml?fileId=7884702&version=1.0>

Data processing: the authors’ reported z-statistics were used directly and the reported coefficients and standard errors were retained. No transformation of test statistics or data was required.



## Chavalarias et al. 2016 (MEDLINE/PubMed)

Reference: Chavalarias et al 2016

Research question: reporting of p-values in the biomedical literature

Data collection: the authors used large-scale text mining of MEDLINE abstracts and PubMed full texts: 4.5mln p-values in 1.6mln MEDLINE abstracts; 3.4mln p-values in 385k PubMed full-text articles.

Data availability: the extracted p-value dataset is publicly available <https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/6FMTT3> (about 6BG of data)
As per the hosting platform: this dataset is released under the Creative Commons Attribution-NonCommercial 4.0 International License. We extracted the relevant columns into `chavalarias.rds` dataset (25 MB).

Data processing: z-values were derived from p-values assuming two-sided tests. P-value truncation operators were processed by collapsing variants such as `<<`, `<<<`, `<=`, `less than`, and `=<` into `<`. We dropped 0.7% of rows where p-values did not have a “plain” format and 0.08% of rows where truncation could not be unambiguously classified.

Additional variables used: source indicator (abstract versus full text).



## clinicaltrials.gov snapshot

Trials which reported results at clinicaltrials.gov

Snapshot date: 12 August 2025

Data source: 
This is a set of flat file of studies obtained via <https://aact.ctti-clinicaltrials.org/> (over 2GBs of data for relevant tables) with relevant columns extracted and joined by us to produce a single table `clinicaltrials.gov_aug2025.rds` (12 MB of data). 

Data availability: terms and conditions for ClinicalTrials.gov are at <https://clinicaltrials.gov/about-site/terms-conditions>. We make distrbituions of derived effect sizes available in accordance with ToC's. We only use publicly available data and do not imply any endorsement of this work by National Library of Medicine, ClinicalTrials.gov or AACT.

Data processing: 
extensive. We kept only studies where status was "Completed" and only rows of data where outcome type was "Primary". We set year to the year of `completion_date`.

Both p-values and confidence intervals were reported. See _Standard procedure for dealing with p-values and confidence intervals_ below for how we created and chose the z-values. Briefly, we calculated z-values using two separate procedures (based on p-values and confidence intervals) and, if both were available, we made a judgement on which one was more appropriate using a pre-defined rule. We dropped rows only when `z` was missing. Notably, infinite z-values can arise when `p_value` is recorded as exactly 0, and these are retained because they are not missing.

The result of our data processing is `data_cut_with_z.rds`, 2.1MB file.



## Cochrane Database of Systematic Reviews

Snapshot: 20 November 2025

Reference: Schwab 2024
  
Data collection: using R package `cochrane` we merged data from 8,726 records of reviews in CDSR and merged them into a single file `cdsr_interventions_19nov2025.csv` (63 MB). 

Data processing: 
minimal processing with extensive filtering. Our processed dataset has 39,768 rows, compared to over 800,000 rows of data in the unprocessed dataset. 

On processing side, we cleaned up study years, categorised measures (risk ratio, odds ratio, mean difference etc.) and experimental designs, especially an "RCT" flag (based on scanning of abstracts of each review for inclusion criteria). Most studies in CDSR are RCTs but some reviews also allowed quasi-experimental studies, which prevented us from categorising many studies.

Unlike in most of the other datasets in BEAR, we filter CDSR data heavily. First, we only use the outcome and comparison of each review that are coded as "1", as that is most likely the primary outcome and most relevant comparison. This removes over 90% of all rows of data. Secondly, we only use studies with continuous and dichotomous outcomes, removing about 10% of data where effect estimates are based on instrumental variables or based on individual patient data. Lastly, we remove about 5% of data where measure of effect is unknown (retaining: OR, RR, Peto OR, mean difference, standardised mean difference, and risk difference). We also remove some rows where there are zero subjects.



## EU Clinical Trials Repository

Trials which reported results at the EU Clinical Trials Register (EUCTR), <https://www.clinicaltrialsregister.eu/>.

Reference: Herold 2025

Snapshot date: October 2025

Data collection: we used R package `ctrdata` to create a snapshot of EU CTR; we downloaded trial records with results only. We stored the downloaded records in a local SQLite database and extracted a fixed set of protocol and results fields plus several fields computed by `ctrdata` (trial phase, sample size, and a “first primary endpoint” p-value/size derived by the package). We saved the extracted flat dataset as `data_euctr_ctgov.rds`, but we only use the EUCTR portion of this combined dataset in BEAR.

Data availability: EUCTR is publicly accessible. The EUCTR legal notice notes that publication on EUCTR does not constitute an endorsement of the information reported. 

Data processing: extensive. We use only “first primary endpoint”. We defaulted missing CI levels to 95%. Rest of the derivation of z-values followed the  _Standard procedure for dealing with p-values and confidence intervals_ described below. We derived `year` from the record date, dropped rows only when z was not finite, and harmonised phase labels to the same naming convention as used for ClinicalTrials.gov.

Additional variables used: we retained phase labels, a measure class (derived from the estimand label), primary endpoint sample size 



## Head et al 2015 (PubMed papers)

Reference: Head et al 2015

Research question: 
extent of p-hacking and its impact on meta-analyses.

Data collection: 
PubMed papers that are open access, up to 2014. Authors used regular expressions to extract p-values from text of Abstract and Results, but not tables. Downloaded data has about 220,000 studies and about 2mln rows.

Data availability: 
dataset available at <https://datadryad.org/dataset/doi:10.5061/dryad.79d43> under CC0 1.0 Universal licence.

Data file: `head.rds` (derived from the Dryad files `p.values.csv` and `journal.categories.csv`, with an additional DOI-to-PMID matching table created by us).

Data processing: we followed the same minimal clean-up steps (e.g. removing values found in large supplementary sections) as the original paper and also attached PubMed IDs to the dataset using DOI look-up. We saved a reduced analysis file `head.rds` (9 MB).

When constructing the BEAR dataset, we treated p-values recorded as exactly 0 as truncated and set the z-operator to `>`. We recoded all inequalities to sharp inequalities.

Additional variables used: we use the "Abstract vs Results" variable for grouping



## Jager and Leek

Reference: Jager and Leek 2014

Research question: estimating science-wise false discovery rate.

Data collection: authors used a custom program to extract p-values from scraped PubMed abstracts for papers published in 5 main medical journals 2000-2010. 15,653 p-values are available in 5,322 articles.

Data availability: the data file `pvalueData.rda` is available <https://github.com/jtleek/swfdr>  License for the programs in that repository is GNU GPL, although a license for the dataset is not stated, as far as we are aware.

Data processing: there was only minimal processing. We derived z values from p-values assuming they were two-sided. 

Notably, large proportion of p-values is truncated, almost always at 0.0001, 0.001, 0.01, or 0.05. As in all datasets, we retained information on truncation. We also treated p-values recorded as exactly 0 as truncated (z-operator `>`). We created a crude flag for RCTs by searching the paper titles for “randomized", "randomised" and "controlled”.



## Open Science Collaboration 2015

Reference: Open Science Collaboration 2015

Research question: 
replications of a quasi-random sample of 100 experiments in psychology.

Data collection: authors of the paper make a comprehensive dataset of results available with their paper. In BEAR we did not use the original studies (since all studies in that set have p < 0.05), only replications.

Data availability: 
the data file `rpp_data.csv` is available as part of repository for the paper <https://osf.io/ytpuq/overview> under CC0 1.0 Universal license. 

Data processing: 
we derived z values from the replication p-values assuming two-sided p-values (we use the replication p-value column, not the original-study p-value). Because the replication dataset does not carry direction in a way we use here, z values are unsigned. We treated p-values recorded as exactly 0 as truncated and recorded this as a lower bound on z (z-operator `>`); otherwise we treated p-values as exact (z-operator `=`). We retained the replication sample size as `ss`.



## Costello and Fox 2022; Yang et al 2023, 2024 (ecology and evolution)

These two datasets come from the same replication package and shared data processing pipeline in:

Reference: Yang et al 2024

Original references are:

Reference: Yang et al 2023

Reference: Costello and Fox 2022


Research question: decline effects in ecology (Costello and Fox) and publication bias and performance of empirical research (Yang et al)

Data collection: Costello and Fox look at 466 meta-analyses obtained through searching Web of Science. Reproducibility dataset includes 88,000 rows among 466 analyses in 232 papers. Subsequently, Yang et al removed duplicated studies and zeroes, which meant discarding about 20% of rows. Yang et al collect 102 meta-analyses published 2010-2019 in relevant journals, de-duplicated against Costello and Fox dataset.

Data availability: two datasets may be downloaded from <https://github.com/Yefeng0920/replication_EcoEvo_git/>. Available under CC BY 4.0 licence. 

Data processing: no additional data processing other than cleaning up study years. Note that the replication package for Yang et al does not include meta-analysis indicators.

Additional variables used: we retained a measure variable ("lnRR", "SMD", "Zr" or "uncommon")



## What Works Clearinghouse (education)

Research question: 


Data availability: 
flat files were obtained from Institute of Education Sciences at <https://ies.ed.gov/ncee/wwc/studyfindings>; snapshot date: 17 May 2025. Per permissions and disclaimers section on that website: "Unless stated otherwise, all information on the U.S. Department of Education’s IES website … is in the public domain and may be reproduced, published, linked to, or otherwise used without permission from IES.”

Data collection: 
WWC datasets have 13,054 findings (data-rows) across 1,908 reviews. The dataset includes effect sizes, but not standard errors, as well as p-values reported by the studies and p-values re-calculated by the WWC reviewers.


Data processing: 
We retained only RCTs and quasi-experimental studies (98% of data altogether). Where available, we preferred effect sizes and p-values calculated by WWC to the ones reported by the studies. We replaced p-values that were equal to zero (this occurred in 3% of the dataset) with truncated p-value (i.e. assumed $p < 10^{-16}$). We calculated z-values from these p-values under assumption of two-sided tests.

Additional grouping variable: 
"outcome domain" variable (e.g. "academic achievement", "alphabetics")




## Converting p-values to z-values



## Standard procedure for dealing with p-values and confidence intervals

This procedure is mainly used for preparing data from clinical trials.

### Cleaning up data

For confidence interval levels, we clean them up by setting values above 100 or below 0 to missing, and treating values below 1 as proportions rather than percentages by multiplying by 100. This means that, for example, 0.95 is treated as a 95% confidence interval. For missing values, we assume 95% (for clinical trials, this matters for EU CTR, but not for clinicaltrials.gov).

In clinical trial datasets we classify the effect measure using text matching to a small set of classes (Mean Difference, Odds Ratio, Risk Ratio, Hazard Ratio, Geometric Ratio, Risk Difference, Difference in Percentages, Ratio/Other Ratio, and Other). This string-based mapping is an arbitrary choice. Unstandardised data have many hundreds of values.

Where possible, we chose the analysis scale based on the measure class. Any ratio measures were analysed on the log scale (if and only if lower bound, upper bound, and point estimate were all strictly positive). Non-ratio measures were always kept on the raw scale.

### Confidence intervals

For z-statistics, we computed the critical value using stated CI percent and the reported number of sides (if missing, we defaulted to a two-sided interval). To avoid undefined critical values (e.g. when at 100% or extremely close to it), we bounded the implied $\alpha$ at a small value. We then computed two one-sided standard error estimates from the upper and lower bounds and used their average when both were finite. 

We computed a simple symmetry diagnostic for the confidence interval on the chosen scale, defined as the smaller of the two half-widths divided by the larger. We treated intervals as “symmetric enough” only if this ratio exceeded 0.8. This threshold is arbitrary and affects when we trust the CI-derived z-statistic.

We also flagged potentially non-Wald intervals using text matching on type of parameter column, if available, searching for keywords such as median, Hodges, posterior/Bayes, exact, Fieller, bootstrap, and permutation. Any row flagged this way was treated as unreliable for CI-based inference, regardless of its numeric properties.

### p-values

The default approach, used for several datasets which report p-values, especially those scraping data from PubMed/Medline, is to treat p-values as two-sided (`z <- qnorm(1 - p/2)`). 

For clinicaltrials databases we find some ambiguity in how p-values were recorded. Sometimes significant results report p-values close to 1, suggesting that authors reported cumunlative distribution, $\Phi(z)$. Therefore for each row we also calculated $2\cdot\min\{p,1-p\}$. When a CI-derived z-statistic was available, we chose whichever p-value led to a z-value that was closest to the CI-derived z. 

We retained unsigned z-statistics when the sign could not be determined. We treated the sign as known only when the transformed effect was finite and non-zero. If the sign was known, we applied it to the p-derived z; otherwise we kept the p-derived z as an absolute value. This choice was explicitly introduced to avoid dropping rows where p-values are available but the sign is unclear.

### Choice of a single z-value

We then chose a single “best” z-statistic per row. We preferred the CI-derived z when it looked like a plausible Wald interval on the chosen scale, meaning: finite z and standard error, positive standard error, not flagged as non-Wald by keyword search, and either missing symmetry information or symmetry above 0.8. Otherwise we fell back to the p-derived z. This decision rule determines which information source dominates and will affect the distribution of z in the final dataset.

## References

Arel-Bundock, V., Briggs, R. C., Doucouliagos, H., Mendoza Aviña, M., & Stanley, T. D. (2022). Quantitative political science research is greatly underpowered. I4R Discussion Paper Series.

Askarov, Z., Doucouliagos, A., Doucouliagos, H., & Stanley, T. D. (2023). The significance of data-sharing policy. Journal of the European Economic Association, 21(3), 1191–1226.

Barnett, A. G., & Wren, J. D. (2019). Examination of CIs in health and medical journals from 1976 to 2019: An observational study. BMJ Open, 9(11), e032506.

Bartoš, F., Lušková, M., Bortnikova, K., Hozová, K., Kantova, K., Irsova, Z., & Havranek, T. (2025). Effect of Exercise on Cognition, Memory, and Executive Function: A Study-Level Meta-Meta-Analysis Across Populations and Exercise Categories. PsyArXiv. https://doi.org/10.31234/osf.io/qr8e2_v1

Brodeur, A., Cook, N. M., Hartley, J. S., & Heyes, A. (2024). Do preregistration and preanalysis plans reduce p-hacking and publication bias? Evidence from 15,992 test statistics and suggestions for improvement. Journal of Political Economy Microeconomics, 2(3), 527–561.

Chavalarias, D., Wallach, J. D., Li, A. H. T., & Ioannidis, J. P. A. (2016). Evolution of reporting p values in the biomedical literature, 1990–2015. JAMA, 315(11), 1141–1148.

Costello, L., & Fox, J. W. (2022). Decline effects are rare in ecology. Ecology, 103(6), e3680. https://doi.org/10.1002/ecy.3680

Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The extent and consequences of p-hacking in science. PLOS Biology. https://doi.org/10.1371/journal.pbio.1002106

Herold, R. (2025). Aggregating and analysing clinical trials data from multiple public registers using R package ctrdata. Research Synthesis Methods, 1–33. https://doi.org/10.1017/rsm.2025.10061

Jager, L. R., & Leek, J. T. (2014). An estimate of the science-wise false discovery rate and application to the top medical literature. Biostatistics, 15(1), 1–12. https://doi.org/10.1093/biostatistics/kxt007

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716

Schwab, S. (2024). cochrane: Import Data from the Cochrane Database of Systematic Reviews (CDSR). https://github.com/schw4b/cochrane

Singh, B., Bennett, H., Miatke, A., Dumuid, D., Curtis, R., Ferguson, T., Brinsley, J., Szeto, K., Petersen, J. M., Gough, C., Eglitis, E., Simpson, C. E., Ekegren, C. L., Smith, A. E., Erickson, K. I., & Maher, C. (2025). Effectiveness of exercise for improving cognition, memory and executive function: A systematic umbrella review and meta-meta-analysis. British Journal of Sports Medicine, 59(12), 866–876. https://doi.org/10.1136/bjsports-2024-108589

Yang, Y., Sánchez-Tójar, A., O’Dea, R. E., Noble, D. W. A., Koricheva, J., Jennions, M. D., Parker, T. H., Lagisz, M., & Nakagawa, S. (2023). Publication bias impacts on effect size, statistical power, and magnitude (Type M) and sign (Type S) errors in ecology and evolutionary biology. BMC Biology, 21(1), 71.

Yang, Y., van Zwet, E., Ignatiadis, N., & Nakagawa, S. (2024). A large-scale in silico replication of ecological and evolutionary studies. Nature Ecology & Evolution, 1–5.
