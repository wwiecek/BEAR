\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,url,epsfig,float,xcolor}
%%% Document layout, margins
\usepackage{geometry} 
\geometry{letterpaper, textwidth=6.5in, textheight=9in, marginparsep=1em}
%%% Section headings
\usepackage{sectsty} 
\usepackage[normalem]{ulem}
%\setlength{\baselineskip}{40pt}

\sectionfont{\sffamily\bfseries\upshape\large}
\subsectionfont{\sffamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\sffamily\mdseries\upshape\normalsize}
\makeatletter
\renewcommand\@seccntformat[1]{\csname the#1\endcsname.\quad}
\makeatother\renewcommand{\bibitem}{\vskip 2pt\par\hangindent\parindent\hskip-\parindent}
\newcommand{\mme}{\mathbb{E}}

\makeatletter
\def\@maketitle{%
  \begin{center}%
  \let \footnote \thanks
    {\large \@title \par}%
    {\normalsize
      \begin{tabular}[t]{c}%
        \@author
      \end{tabular}\par}%
    {\small \@date}%
  \end{center}%
}
\makeatother


\usepackage{mathtools}
\usepackage{extarrows}
\usepackage{enumerate} 
\usepackage{hyperref}
\usepackage{bbm}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=black,
}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage[toc,page]{appendix}
\newcommand{\note}[1]{{\color{red}#1}}
 \usepackage{comment}
 \usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{microtype}
 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{relsize}
\let\oldv\verbatim
\let\oldendv\endverbatim
\def\verbatim{\par\setbox0\vbox\bgroup\oldv}
\def\endverbatim{\oldendv\egroup\fboxsep0pt \noindent\colorbox[gray]{0.96}{\usebox0}\par}
%%%%%
 % \startlocaldefs

 % \endlocaldefs

\DeclareMathOperator{\SNR}{\mathrm{SNR}}
\DeclareMathOperator{\normal}{\mathrm{normal}}
\DeclareMathOperator{\MVN}{\mathrm{MVN}}
\DeclareMathOperator{\uniform}{\mathrm{uniform}}
\DeclareMathOperator{\Poisson}{\mathrm{Poisson}}
\DeclareMathOperator{\exponential}{\mathrm{exponential}}
\DeclareMathOperator{\binomial}{\mathrm{binomial}}
\DeclareMathOperator{\Cauchy}{\mathrm{Cauchy}}
\DeclareMathOperator{\logit}{\mathrm{logit}}
\DeclareMathOperator{\logistic}{\mathrm{logistic}}
\DeclareMathOperator{\lognormal}{\mathrm{lognormal}}
\DeclareMathOperator{\E}{\mathrm{E}}
\DeclareMathOperator{\Var}{\mathrm{var}}
\DeclareMathOperator{\sd}{\mathrm{sd}}
\DeclareMathOperator{\Cov}{\mathrm{cov}}
\DeclareMathOperator{\Dev}{\mathrm{Dev}}
\DeclareMathOperator{\Beta}{\mathrm{beta}}
\DeclareMathOperator{\mode}{\mathrm{mode}}
\renewcommand{\Pr}{\mathrm{Pr}}


\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{% general command to set parameter(s)
  basicstyle=\small\ttfamily
  }


\usepackage[style=authoryear-comp,indexing=cite]{biblatex}
\ExecuteBibliographyOptions{sortlocale=general,maxnames=12,maxcitenames=3,parentracker=true,sortcites=false,uniquename=false,dashed=false,hyperref=true}
\uspunctuation
\addbibresource{references.bib}
\renewbibmacro{in:}{}


\begin{document}\sloppy

\title{\bf A Statistical Case for Qualified Scientific Optimism\vspace{.1in}}

\author{
Erik van Zwet\thanks{Department of Biomedical Data Sciences, Leiden University Medical Center.}
\and Andrew Gelman\thanks{Department of Statistics and Department of Political Sciences, Columbia University, New York.}
\and Witold Wiecek\thanks{Development Innovation Lab, University of Chicago.}
\vspace{.1in}
}

\date{30 Dec 2025}

\maketitle

\begin{abstract}

We create an open-source database of nearly 100,000 empirical research results, combining 12 publicly available curated corpuses of studies in medicine, economics, psychology, psychotherapy, ecology, evolution, political science, education, and more, and available at \url{https://github.com/wwiecek/BEAR}. Each result is represented by a $z$-score:  an estimated effect divided by its standard error.

We then provide a modeling framework that allows us to move away from the usual attempts to classify effects as exactly zero or not or to classify replications as successful or not, and instead focus on identifying the direction of effects and their replicability. 

After adjusting for publication bias, we find two patterns that hold across all 12 datasets and various subsets: (1) statistical significance does not imply a high probability of successful replication, but (2) it does imply a high probability that the direction of the observed effect was correct. 

We conclude that replication rate is a poor measure of the quality of a field of research and argue that an attitude of global scientific pessimism based on low replication rates is misguided. Instead, we favor a more optimistic view of scientific research based on the ability to identify the direction of effects. This optimism is not naive about the many challenges science continues to face such as questionable research practices (QRPs), paper mills, fake AI papers and such. 

\end{abstract}

\section{Studying the quality of empirical science}\label{introduction}

The replication crisis has made us aware that there's a wide range of quality in the human sciences, even considering subsets such as particular fields and papers published in top journals.  This makes sense: if we roughly measure the quality of a study by its signal-to-noise ratio (the true effect size divided by its standard error), then we have some control over the denominator (by taking more or better measurements) but less control over the numerator:  it's the nature of research to be uncertain and it's the duty of researchers to pursue all avenues, studying effects which might turn out to be null or highly context-dependent.

A key turning point in the replication crisis was the recognition and refutation of the false intuition that you can tell whether a study is good or not (tell whether a result is strong or not) using a statistical significance threshold.  Even beyond the obvious problems with any sharp threshold, even clear distinctions such as, for example, $p < 0.01$ compared to $p > 0.2$ do not discriminate well between high and low signal-to-noise ratios,\footnote{The two-sided $p$-values of 0.01 and 0.2 correspond to absolute $z$-scores of 2.58 and 1.28, respectively. These are about equally likely when the signal-to-noise ratio is $(2.58+1.28)/2=1.93$. This signal-to-noise ratio corresponds to a little under 50\% power ($\Pr(|z| \geq 1.96 \mid \text{SNR}=1.93) = 0.49$).} and similar problem arises not just with $p$-values but also with Bayes factors and other measures of quality of evidence. 

The message of the present paper is that the difficulty of assessing the signal-to-noise ratio for any given study should not be taken as a negative verdict on statistically-based science as a whole.

To study this, we have created an extensive open-source database of empirical research results, composed of 15 publicly available datasets of studies in diverse fields.  Our optimistic finding is that, in these corpora, most results seem to be correct in sign.  Our remaining pessimism is that is that you can't learn so much from any individual study, which points toward the need for real replication--not in the hope of confirming significance but to understand effects, and to minimize selection in all phases of the process.

\subsection{Going beyond the characterization of scientific claims as true or false}

In an influential paper, \textcite{ioannidis2005most} modeled empirical research as ``a $2 \times 2$ table in which research findings are compared against the gold standard of true relationships in a scientific field.''
%Assuming statistical tests are done at significance level $\alpha=0.05$ (two-sided), Ioannidis characterizes a research field in terms of the ratio of the number of ``true relationships'' to ``no relationships'' $R$ and the power, $1 - \beta$, the probability of obtaining a statistically significant result conditional on there being a true relationship. He proceeds to derive expressions for the $2 \times 2$ table of proportions of true and false positive and negative findings, which is also sometimes called the confusion matrix. Among these proportions, he emphasizes the  positive predictive value (PPV), the post-study probability of a research finding being true, conditional on its estimate achieving formal statistical significance. He finds $\mathit{PPV} = (1 - \beta)R/(R - \beta R + 0.05)$ and concludes that a research finding is more likely true than false if $(1 - \beta)R > 0.05$.

This $2 \times 2$ model is implicitly accepted in the meta-scientific literature, which tends to focus on statistical significance and replication rates \parencite{simmonsFalsePositivePsychologyUndisclosed2011, opensciencecollaborationEstimatingReproducibilityPsychological2015a, camerer2016evaluating}. In particular, low replication rates are taken by many as an evidence of published empirical research being full of null claims and hence not credible as a whole, a position with political influence; see, for example, \textcite{garisto2025trump}.

We understand that the $2 \times 2$ model was always intended as an instructive simplification, introduced for the purpose of explaining that in an area of research where the power is low and there are few true relationships, many statistically significant results may not be true. However, far-reaching conclusions from this model have been made about the actual state of scientific research, in particular the claim that more than half of all published research findings are false.

We have two main criticisms of the $2 \times 2$ model. First, we disagree with the notion that the primary goal of scientific study is to separate ``no relationships'' or nulls from ``true relationships.''  Effects of interest are very rarely exactly zero, and even if they are, they will no longer be zero if one takes leakage, expectation effects, dropout, etc. into account. But apart from that, with finite data, it is simply not possible to distinguish effects that are zero from those that are small.

Our second criticism is that the $2 \times 2$ model does not comport with data we have collected on published research. We have compiled a corpus of 15 datasets, each containing thousands of studies from medicine, economics, psychology, psychotherapy, ecology, evolution, political science, education, and more. We have used these data to build more realistic models of research areas.

% Our second criticism is that the $2 \times 2$ model is not empirical, i.e. it is not based on data---and because we cannot statistically distinguish small and zero effects, it could not be well-identified from data anyway.
% %WW: I think currently we say what we do twice ("we have compiled" and then "contribution is two-fold"), so I tried to reorganise and add this bit of context:
% And while in the broader field of metascience some excellent quantitative work has been done since \parencite{opensciencecollaborationEstimatingReproducibilityPsychological2015a}, the broader understanding of quality of empirical research is fragmentary and based on small samples of studies.

In this paper we make two contributions. First, we make a large number of results from empirical research easily accessible in a new database, Benchmarks of Empirical Accuracy in Research (BEAR). We do not contribute any new data, but we process the available data to make them ready for meta-scientific research. We provide clear documentation and all data are available under open source license at \url{https://github.com/wwiecek/BEAR}. We will continue to add new datasets as they become available.

Second, we extend the meta-scientific model of \textcite{van2021statistical,zwet2022proposal} and \textcite{vanzwet2022large}  to account for truncation and publication bias. This model allows us to study measures of research quality that are insensitive to whether effects are exactly zero or merely very small, such as the probability of correctly identifying the direction of an effect and the probability of replication \parencite{greenland2017invited, gelman2014beyond, gelman2000type}. 

\subsection{Summary of findings}

We estimate from our analysis that statistical significance usually implies a high probability that the direction of the observed effect is correct, but that it does not imply a high probability that an exact replication study will reach statistical significance. In other words, it is expected that true findings (in the sense of correctly identifying the direction of an effect) will often not replicate. Similar conclusions were reached by \textcite{bak2022revisiting} and \textcite{neves2022most} on the basis of simulation models.

The scientific process is far from perfect, and questionable research practices, paper mills, and outright fraud are very serious concerns \parencite{aquarius2025tackling,matusz2025threat}. But the mere fact that many studies have low probability of reaching statistical significance and therefore fail to replicate does not imply that most published research findings are false. 

The challenge is internalizing the juxtaposition of two ideas.  On one hand, there really is a replication crisis:  substantive and statistical theory, empirical evidence, and sociological reasoning all point toward the conclusion that traditional procedures for scientific quality control--peer review, causal identification, and statistical significance--are not enough to ensure replicability or to bring us closer to an understanding of reality.  On the other hand, the statistical evidence implies that in many fields, published results generally fall in the correct direction.

%Evidence for the replication crisis:
%\begin{itemize}
%\item Substantive theory:  Many heavily-promoted claims (extra-sensory perception, power pose, mind-body healing, shark attacks, etc.) defy common sense.  This is not to say that they \emph{can't} be true, just that there is legitimate skepticism about them. Also, as discussed in the piranha paper, these claims can't all coexist.
%\item Statistics:  Researcher degrees of freedom, forking paths, type M and S errors, reasons why we can't take statistically significant findings at face value.
%\item Empirical evidence:  Various individual studies that did not replicate (elderly walking, etc.), also large-scale replication studies.
%\item Sociological reasoning:  Incentives for big claims, the feedback loop of overestimates and overconfidence.
%\end{itemize}
%A key takeaway from the replication crisis that we shouldn't automatically trust published results, even if they come from a respected institution and are published in a top journal.


\section{Data}

We have compiled a large number of empirical results from a variety of sources in a new database which we call Benchmarks of Empirical Accuracy in Research (BEAR). The online version of BEAR includes 20 datasets and 11.5 mln z-values from empirical research, but for this paper we focus on about 100,000 z-values from 12 curated datasets of research. We also show two sets of replications and two large-scale reviews which scraped p-values from abstracts for contrast.   Characteristics of presented datasets are given in Table~\ref{tab:dataset_summary} and we will refer to the datasets by their names from that table. 

For the 12 curated, domain-specific collections, we include: 
a large collection of mainly clinical trials which posted results on {\tt clinicaltrials.gov} or EU Clinical Trials Repository, 
a subset of 30,000 trials from Cochrane database (CDSR, \textcite{schwab2020}), which is well-curated and large (over 400,000 data rows), and 13,000 studies in ecology and evolution \parencite{costello2022decline,yang2024large}. 

Other datasets (each including from hundreds to about 2,000 studies) are 
education research from the What Works Clearinghouse, %add cite here
% cover psychology \parencite{sladekova2023estimating,bartovs2023meta}, 
psychotherapy (the MetaPsy project, \textcite{cuijpers2024effects}), 
psychology %cite psymetadata
% ecology and evolution \parencite{costello2022decline,yang2023publication,yang2024large}, 
exercise %cite Bartos
intelligence %cite Nuijten
economics \parencite{askarov2023significance, brodeur2024preregistration}, 
and political science \parencite{arel2022quantitative}. 

Eight of these 14 are sets of meta-analyses, but in this paper we treat them as collections of individual studies. This meta-analytic structure is available in BEAR and may be useful for other metascientific research, for example characterizing heterogeneity in treatment effects \parencite{single_meta}. 

The majority of datasets already include z-values or effect sizes and standard errors, but in some cases we make calculations ourselves (for example, from p-values or counts for binary data); calculation details are in Appendix B. The average number of estimates per study for each dataset is given in Table 1; it can be as high as 50 estimates per study in \textcite{arel2022quantitative}, which aimed to collect all tests reported by economics papers, but in most cases we have either one or several estimates per study.

For comparison we also include two replication projects in psychology: 
%studies from the Open Science Collaboration's effort to estimate the  reproducibility of psychology \parencite{opensciencecollaborationEstimatingReproducibilityPsychological2015a}
 \parencite{opensciencecollaborationEstimatingReproducibilityPsychological2015a} %add cite to Many Labs 2
% Third, we have four large-scale reviews which were automatically collected (scraped) from abstracts or full texts available via Medline and PubMed. \textcite{chavalarias2016evolution} and \textcite{head2015extent} provide collections of $p$-values, while \textcite{barnett2019examination} have effect estimates of binary outcomes (odds ratios, risk ratios and such) and their confidence intervals. These three data sources are all very large with hundreds of thousands of studies and millions of estimates. We also include a smaller dataset from \textcite{jager2014estimate} in this category since it relied on scraping $p$-values of papers in leading medical journals. There is considerable overlap among these four datasets.
We also contrast our results with two large-scale reviews which were automatically collected (scraped) from abstracts or full texts available via Medline and PubMed: \textcite{chavalarias2016evolution} provide collections of $p$-values, while \textcite{barnett2019examination} have effect estimates of binary outcomes (odds ratios, risk ratios and such) and their confidence intervals. 
%These three data sources are all very large with hundreds of thousands of studies and millions of estimates. We also include a smaller dataset from \textcite{jager2014estimate} in this category since it relied on scraping $p$-values of papers in leading medical journals. There is considerable overlap among these four datasets.

% Our consolidated database has, for each published estimate, the data source, $z$-statistic, $p$-value, effect size estimate, standard error, sample sizes, year of publication, study type (e.g., randomized trial, observational study), measure type (e.g., difference of means, log risk ratio), and some categorization into fields of study and subgroups (e.g., medical field, clinical study phase). The current version of BEAR is a single table with xx rows. 

% In some cases we have access to summary data from which we can calculate effect estimates ourselves, but in most datasets we do not know the underlying method of calculation. Where only $p$-values are available, we assume they are two-sided and calculate the $z$-statistic as $z = \Phi^{-1}(1 - p/2)$. In four of the datasets, the $p$-values or $z$-statistics are truncated if researchers reported only $p < 0.05$, $p < 0.001$, etc. We account for this in our statistical model. 



\input{table1.tex}

\section{Statistical model}

We extend the model of \textcite{van2021statistical,zwet2022proposal,vanzwet2022large} to account for the truncation and publication bias which are conspicuously present in some of our datasets. The main goal is to estimate a latent distribution of the signal-to-noise ratios across a field of research and then use this distribution to derive various aspects of the sampling distribution of replication results.

\subsection{Signal-to-noise ratios}
 
Suppose that we have a collection of unbiased effect estimates which are normally distributed with known standard errors. The $z$-statistic is the ratio of the effect estimate to its standard error. We define the signal-to-noise ratio (SNR) as the ratio of the (unobserved) true effect to the standard error of its estimate. Our assumptions imply that the $z$-statistic is equal to the SNR plus an independent standard normal error. 

The null hypothesis that there is no effect is equivalent to the hypothesis that the SNR is zero. This hypothesis is commonly rejected when the absolute value of the $z$-statistic exceeds 1.96 or, equivalently, when the associated $p$-value is less than 0.05. The type I error probability of this (two-sided) test is $\alpha=0.05$. In our notation, 
$$\Pr(|z| \geq 1.96 \mid \text{SNR}=0)=0.05.$$ 
The probability of statistical significance (PoS) depends on the magnitude of the SNR,
\begin{equation}
\text{PoS}(\text{SNR})=\Pr(|z| \geq 1.96 \mid \text{SNR}) = \Phi(-1.96 - |\text{SNR}|) + 1 - \Phi(1.96 - |\text{SNR}|). \label{eq:power}
\end{equation}
For example, if the $\text{SNR}=2.8$ (or $-2.8$), then the PoS is 80\%. Averaging the PoS over the distribution of the SNR in a field of research, we obtain the ``assurance''  which we denote by $\overline{\text{PoS}}$, the estimated proportion of studies for that field that reaches the 5\% level of statistical significance.  We can also compute $\text{Pr}(PoS>0.8)$, the estimated proportion of studies for which the power relative to the true effect size.

The variability of $p$-values can be easily understood by considering $z$-statistics. A study with 80\% power has SNR of 2.8. In that case, the sampling distribution of the $z$-statistic is normal with mean 2.8 and unit variance. So, both $z=0.8$ ($p=0.42$) and $z=4.8$ ($p=1.6 \cdot 10^{-6}$) can easily occur. 

\subsection{A model for the distribution of signal-to-noise ratios in a corpus}

We will use maximum likelihood to estimate the distribution of SNRs given a sample of absolute $z$-statistics. Our assumptions imply that $z$-statistics are equal to SNRs plus independent standard normal errors. We can therefore obtain the distribution of the absolute SNRs by deconvolution of the standard normal error component from the distribution of the absolute $z$-statistics \parencite{efron2016empirical,stephens2017false}. 

We construct the likelihood in four stages. First, we introduce a mixture distribution for the absolute $z$-statistics. Then, we add a parameter to account for publication bias. Next, we account for the fact that reported values are sometimes truncated. Finally, we weight the observations according to the number of $z$-statistics per study.

Our main assumption is that the distribution of the absolute $z$-statistics is well represented by a mixture of half-normal distributions. We will informally verify this assumption by inspection of the observed histograms. Noting that this assumption is equivalent to modeling the signed $z$-statistics as a mixture of zero-mean normal distributions, we have the mixture density,
\begin{equation}
f(z \mid p,\sigma) = \sum_{i=1}^{k} p_i\,\frac{1}{\sigma_i}
      \,\phi\!\left(\frac{z}{\sigma_i}\right),                  \label{eq:mixture}
\end{equation}
where $\phi(\cdot)$ is the standard normal density,   $p=(p_1,\dots,p_k)$ are the non-negative mixture weights that sum to unity and  $\sigma=(\sigma_1,\dots,\sigma_k)$ are the standard deviations. These standard deviations are at least 1 because we know that $z$-statistics have a standard normal noise component. We will use $k=4$, as we have verified that larger values of $k$ yield nearly identical results. 

The deconvolution to obtain the distribution of the SNRs is easy. We simply subtract 1 from the variances of the mixture components. So,
\begin{equation}
f_\text{SNR}(x \mid p,\sigma) = f(x \mid p,\sqrt{\sigma^2 -1}).
\end{equation}

To account for publication bias, we introduce a simple selection component in the likelihood. We follow \textcite{hedges1984estimation,hedges1992modeling} and define $\omega$ as the odds that a result with $|z|<1.96$ is observed
\begin{equation}
\omega \;=\;
\frac{\Pr(\text{result is published} \mid |z| <1.96)}
     {\Pr(\text{result is published} \mid |z| \ge1.96)}.
\end{equation}
\noindent
The probability of publication is
\begin{equation}
C(p,\sigma,\omega) = \omega\,\Pr(|z| < 1.96 \mid p,\sigma) + \Pr(|z| \geq 1.96 \mid p,\sigma)
\end{equation}
and the density of the $z$-statistics conditional on publication becomes

\begin{equation}
f(z \mid p,\sigma, \omega, \text{pub}) = 
\begin{cases}
\omega\,f(z \mid p,\sigma) / C(p,\sigma,\omega) \quad   & |z| \leq 1.96 \\
      f(z \mid p,\sigma)/ C(p,\sigma,\omega)                              & \text{otherwise.}
\end{cases}
\end{equation}

The $z$-statistic is sometimes left or right truncated, most often at 1.96, but also at, for example, 2.57 (when papers report $p < 0.01$), 3.29 ($p < 0.001$), or 1.64 ($p < 0.1$). To take this into account, we introduce a censoring indicator $\delta$ taking the values $<$, $>$, or $=$ for left censoring, right censoring, or no censoring, respectively. We denote the (possibly) censored value of the $z$-statistic by $\tilde{z}$. The distribution of the pair $(\delta,\tilde{z})$ conditional on publication is
\begin{align*}
&f(\tilde{z},\delta \mid p,\sigma, \omega, \text{pub})= \\
&\begin{cases}
\omega\,f(\tilde{z}\mid p,\sigma)/ C(p,\sigma,\omega)         & \delta=\texttt{"="},\; \tilde{z}<1.96,\\[4pt]
      f(\tilde{z}\mid p,\sigma)/C(p,\sigma,\omega)              & \delta=\texttt{"="},\; \tilde{z}\ge 1.96,\\[6pt]
\omega\,F(\tilde{z}\mid p,\sigma)/ C(p,\sigma,\omega)            & \delta=\texttt{"<"},\; \tilde{z}< 1.96,\\[4pt]
(\omega\,F(1.96\mid p,\sigma)+F(\tilde{z}\mid p,\sigma)-F(1.96\mid p,\sigma))/ C(p,\sigma,\omega)          & \delta=\texttt{"<"},\; \tilde{z}\ge  1.96,\\[6pt]
(1-F(\tilde{z}\mid p,\sigma))/ C(p,\sigma,\omega)          & \delta=\texttt{">"},\; \tilde{z}\ge 1.96,\\[4pt]
((1-F(1.96\mid p,\sigma))+\omega\,(F(1.96\mid p,\sigma)-F(\tilde{z}\mid p,\sigma)))/ C(p,\sigma,\omega)
                                               & \delta=\texttt{">"},\; \tilde{z}<  1.96.
\end{cases}
\end{align*}
\noindent
The first two rows cover the case without truncation. Rows three and four are left censoring (statements such as $p > 0.05$), and  rows five and six deal with right censoring (e.g., $p < 0.001$). We also introduce artificial censoring when $z$-statistics are reported to be exactly zero, by treating them as $|z| < 0.5$.

Most of the datasets have multiple $z$-statistics per study. To prevent studies with many $z$-statistics from dominating the likelihood, we weight observations by $w_j$, the inverse of the number of $z$-statistics originating from the study $j$. This does not account for the dependence among $z$-statistics from the same study. However, this does not concern us because we do not attempt to quantify the uncertainty of our estimates.  The reason is that our datasets are so large that the sampling uncertainty which determines standard errors and confidence intervals, is negligible compared to model uncertainty. The full weighted log likelihood is  
\begin{equation}
\ell(p,\sigma, \omega)
= \sum_{j=1}^{n} w_j \log f(\tilde{z_j},\delta_j \mid p,\sigma, \omega, \text{pub}).               \label{eq:loglik}
\end{equation}
\noindent
Even for large datasets, numerical optimization of this log likelihood is feasible, taking just minutes.  Very large datasets with hundreds of thousands or even millions of observations could be randomly subsampled to, say, 50,000 observations---but we would see no reason to do this, because if such a large corpus were available, it would make sense to break it up and estimate the distribution of SNRs separately for each sub-corpus.

The estimated distribution of the $z$-statistics without publication bias is $f(z \mid \hat p,\hat \sigma)$ as defined in (\ref{eq:mixture}) and consequently the estimated distribution of the SNRs is
\begin{equation}
f_\text{SNR}(x \mid \hat p,\hat \sigma) = f(x \mid \hat p,\sqrt{\hat \sigma^2 -1}).
\end{equation}

We can derive many important quantities from the distribution of the SNRs. For example, the PoS is a function of the SNR, so we can obtain its distribution by transforming the distribution of the SNRs. In particular, the proportion of studies that have at least 80\% probability of reaching statistical significance is
\begin{equation}
\Pr(\text{PoS} \geq 0.8) = \Pr(|\text{SNR}| \geq 2.8).
\end{equation}
The marginal density of the SNRs, together with the conditional density of the $z$-statistics given the SNRs, determines the joint distribution. And from the joint  distribution we can derive the conditional distribution of the SNR given the $z$-statistic.
We provide the formulas in the Appendix. In turn, this allows us to derive the conditional probability of the correct sign,
\begin{equation}\label{eq:sign}
\text{Probability of the correct sign} = \Pr(z \cdot \text{SNR} > 0 \,\big|\, |z|),
\end{equation}
Finally, we imagine that a second replication study with exactly the same SNR yields another $z$-statistic $z_\text{repl}$. This  $z_\text{repl}$ is conditionally independent of the original $z$ given SNR, and has the same distribution. Thus \textcite{vanzwet2022large} were able to derive the conditional distribution of $|z_\text{repl}|$ given $|z|$. In particular, we obtain 
\begin{equation}\label{eq:repl}
\text{Probability of ``successful replication''} = \Pr(z_\text{repl} \cdot z > 0 \text{ and } |z_\text{repl}| > 1.96 \,\big|\, |z|) 
\end{equation}

All the computations that yielded the results reported in the next section are documented in the online supplement to this paper. 

\section{Results}

Figure~\ref{fig:zmixtures} displays the histograms of the observed absolute $z$-statistics. Two features are immediately noticeable. First, the histograms of the scraped datasets of \textcite{chavalarias2016evolution}, \textcite{head2015extent} and \textcite{jager2014estimate} have a number of large peaks which are due to truncation. Second, almost all of the datasets show signs of publication bias, but it is most severe by far in the scraped datasets. The scraped dataset of  \textcite{barnett2019examination} is based on reported confidence intervals, so it is possible that some non-significant results were reported, but not with their confidence intervals. This may partly explain the extreme lack of non-statistically-significant $z$-statistics in that dataset. 

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{fig/mixtures_plot.pdf}
\caption{Histograms of absolute $z$-statistics across 15 datasets. The solid curves are the fitted mixtures of half-normal distributions conditional on publication, while the dashed curves are the unconditional distributions.}\label{fig:zmixtures}
\end{figure}

It is tempting to focus on the extent of publication bias, which may in principle be quantified by the estimated relative risk of publication $\hat \omega$ as reported in Table~\ref{tab:results_summary}. However, there are several reasons why we must resist that temptation. First, many studies in our datasets have multiple outcomes, and we do not know which are the primary or headline results that would be more subject to publication bias. Second, in many cases, we have computed the $z$-statistics and $p$-values from reported effect estimates and standard errors. The researchers may have used some other statistical method to perform their statistical tests so that the statistical significance of the results that was actually reported may not coincide with our $z$-statistics exceeding 1.96. Third, the single-parameter selection model is useful in adjusting our estimates of the distributions of the $z$-statistics, but we do not believe it is suitable for quantifying and comparing the extent of publication bias across fields of research. 

The solid curves in Figure~\ref{fig:zmixtures} are the fitted mixtures of half-normal distributions conditional on publication, while the dashed curves are the unconditional distributions.  We find that the estimated distributions generally track the histogram well and conclude that our model is appropriate for our purpose of quantifying large scale properties of research areas.

The fitted mixtures of half-normal densities are necessarily decreasing. Apart from the effects of publication bias, we also see this feature in the histograms of the absolute $z$-statistics. We claim that this is typical of large collections of $z$-statistics. To further support this claim, we have broken down the Cochrane data by 19 medical specialties and the data from {\tt clinicaltrials.gov} by trial phase; see Figures \ref{fig:subsets_cdsr} and \ref{fig:subsets_ct}. Again, we observe a similar decreasing pattern.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/mixtures_plot_cochrane_4.pdf}
    \vspace{-.2in}
    \caption{Subsets of Cochrane database grouped by specialty (out of 19 included in the dataset): two with lowest $\text{E}(|z|)$ on the left and two with the highest, further restricted to RCTs only, on the right.}
    \label{fig:subsets_cdsr}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig/mixtures_plot_clintrials_4.pdf}
    \vspace{-.2in}
    \caption{Subsets of interventional treatment studies from {\tt clinicaltrials.gov} broken down into four phases of clinical trials.}
    \label{fig:subsets_ct}
\end{figure}

The main difference between the distributions of the $z$-statistics in Figures \ref{fig:zmixtures}-\ref{fig:subsets_ct} is their widths. A wider distribution of $z$-statistics implies a wider distribution of SNRs. In other words, the studies in that research area tend to have larger absolute SNRs. Since the PoS is an increasing function of the absolute SNR, this also means that a higher proportion of results will reach statistical significance. Recall that the assurance $\overline{\text{PoS}}$ is the proportion of significant results, adjusted for publication bias; see (\ref{eq:power}). We report the assurance in each of the Figures and also in Table~\ref{tab:results_summary}. The assurance is less than the raw proportion of significant results, unless $\hat{\omega}=1$ (i.e.\ no discernible publication bias). 

By transforming the distribution of the signal-to-noise ratios, we obtain the full distribution of the PoS. The left panel of Figure~\ref{fig:pos_gap} displays the cumulative distribution functions of the probability of statistical significance for each of the 15 datasets. The $2 \times 2$ model assumes that this distribution is a mixture of point masses on 0.05 (null effects) and some other value such as 0.8 (non-null effects). Evidently, that is not the case.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{fig/pos_gap.pdf}
\vspace{-.2in}
\caption{Left panel: Cumulative distribution functions of the probability of statistical significance for each of the 15 datasets. Right panel: Conditional probability that observed direction of the effect matches the true direction (blue curves) and the conditional probability of ``successful replication'' (red curve).}\label{fig:pos_gap}
\end{figure}

From the distribution of the PoS, we can derive the proportion of studies where the PoS is at least 0.8. We report these in Table~\ref{tab:results_summary}. Since the great majority of studies are targeted at reaching 80\% power, this may be tentatively interpreted as the proportion of studies where the assumptions of the sample size calculation were true. In general, this proportion is low. However, sample size calculations are often not targeted at the true effect (which is never known) but at the effect one would not want to miss. In that sense, a low proportion of studies with 80\% PoS just means that it is hard to come up with  interventions that have large effects. On the other hand, it is also true that pressures of time, money and logistics tend to drive down sample sizes.   

Figure~\ref{fig:pos_gap} shows the conditional probabilities of the correct sign and ``successful replication'' as defined in equations (\ref{eq:sign}) and (\ref{eq:repl}). The horizontal dashed lines are the assurances. When the $z$-statistic exceeds 1.96, the probability of the correct sign is typically well above 90\%. At the same time, the probability of ``successful replication'' is very small, reaching 80\% or 90\% only for extremely large values of $|z|$. 

In Table~\ref{tab:results_summary} we report the probabilities of the correct sign and ``successful replication'' when  the $z$-statistic is equal to 1.96. In that case, the probability of ``successful replication'' is approximately equal to the assurance, indeed a bit smaller. In other words, if a result is marginally statistically significant at 5\% level, the probability of ``successful replication'' is not increased compared to the base rate. This will be very surprising to those who think of statistical significance as a stand-in for replication. This phenomenon is {\em not} a consequence many studies having low SNRs (low PoS). If all the studies had higher SNRs, then the assurance would be higher, but observing $|z|=1.96$ would be even less impressive.
 

\input{table2.tex}

\section{Discussion}

The  $2 \times 2$ model is often taken to be a reasonable, if simplified, description of scientific research. According to this model, the probabilities of significance (PoS) of the ``true relationships'' and ``no relationships'' are well separated. The PoS of ``no relationships'' is 5\% by the design of the statistical test, and the PoS of ``relationships'' is much larger by the design of the experiment. If the ratio of ``true relationships'' to ``no relationships'' is not too low, then we can reliably separate them. That is, the positive predictive value (the conditional probability of a true relationship given statistical significance) and the negative predictive value (the conditional probability of no relationship given non-significance) are both high. Moreover, the conditional probability of successful replication (reaching statistical significance again in an exact replication study) is also high.

We have developed an alternative statistical model in the context of the Cochrane database of systematic reviews \parencite{van2021statistical,zwet2022proposal}.  Our model goes beyond the $2\times 2$ framework by allowing the PoS to range continuously from 0.05 to 1. Here we extend this model by incorporating the \textcite{hedges1984estimation, hedges1992modeling} selection model to adjust for publication bias. We also allow for truncation. We fit our model to 15 publicly available datasets, each containing thousands of studies from diverse fields.
%: medicine, economics, psychology, psychotherapy, ecology, evolution, political science and education. %avoiding repeat
The model fits well; see Figures~\ref{fig:zmixtures}--\ref{fig:subsets_ct}.  In contrast, the $2 \times 2$ model does not fit our data, as we observe no clear separation between the probability of statistical significance for ``true relationships'' and ``no relationships.''

Our model allows us to move away from the paradigm of ``no relationships'' versus ``true relationships'' and the associated idea of type 1 and 2 errors. Instead we focus on replication and identifying the direction of effects which are both insensitive to whether effects are zero or merely small. Our main finding is that statistical significance usually implies a high probability that the direction of the observed effect is correct, but that it does not imply a high probability that an exact replication study will be significant again, see Figure~\ref{fig:pos_gap}. In other words, we often have true findings (in the sense of correctly identifying the direction of an effect) that do not replicate. Therefore, failure to replicate should not be taken to mean that the original result was a fluke or a fake.

This should not come as a surprise, as the difference between ``significant'' and ``not significant'' is not itself statistically significant \parencite{gelman2006difference}. And yet, it can be viewed as scandalous when a statistically-significant published result does not replicate \parencite{tversky1971}. This suggest a widespread discrepancy between expectations and reality which we believe to be due to the unrealistic $2 \times 2$ model that people have in mind.

The current pessimism in the meta-scientific literature reflects a one-sided interpretation of Figure~\ref{fig:pos_gap} focusing only on the low replication rates. This skewed view has serious consequences as it may lead to dangerous nihilism among policy makers \parencite{garisto2025trump}. We argue for a more balanced view where we also take into account the high probabilities of getting the sign right.

While there are clear differences between research fields in terms of the distribution of the SNR, most are similar. We tentatively ascribe this to some universal anthropic principle whereby researchers respond to the pressures of time, money and logistics on one side and the goal to reach statistical significance on the other, by making their studies just about large enough to assess the direction of the effect of interest, but not larger.


We need to strike a balance . . .


It depends on the subfield! . . .


\section*{To keep in mind}

{\bf Disclaimer}: Our results apply to repeated sampling of studies (or a random sample of a study) from certain research fields. To apply them to a particular study, would require us to ignore everything about that study except the observed $z$-statistic.

\medskip\noindent
{\bf Main message 1}: Empirical finding: research areas are similar in the sense that they have decreasing distributions of absolute $z$-statistics (and hence decreasing distributions of absolute SNRs). This is an anthropogenic/anthropic phenomenon.

\medskip\noindent
{\bf Main message 2}: Individual studies can be bad or untrustworthy for many reasons. However, a replication rate of 30-40\% of a research area is no reason for mistrust. It's merely a consequence of the inherent variability of $z$-statistics, i.e. they're the convolution of some distribution that is mostly supported on $[-2,2]$ (the SNRs) and the standard normal.

\bigskip
The posts on statmodeling about $z$ and SNR show that there’s a lot of confusion. The main issues seem to be:
\begin{itemize}
\item Confusing the SNR with the effect. Even if the effect of some intervention is large, the distribution of $|\SNR|$ can still be decreasing.
 
\item Confusing selection on large $z$ (bad) with selection on large SNR (good).
 
\item Confusing the distribution of z across some field of study with the conditional distribution of z given SNR. The former is not supposed to look normal. In the absence of selection on $z$, it’s only supposed to be as smooth as N(0,1). That the distribution of $|z|$ is often decreasing is an empirical finding.
\end{itemize}
We’ll need to work very hard to explain these things! Maybe think of some good visuals?


\appendix
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0}
\section{Appendix}

Suppose that the marginal distribution of the $z$-statistic is a mixture of zero-mean normal distributions,
\begin{equation*}
f(z \mid p,\sigma) = \sum_{i=1}^{k} p_i\,\frac{1}{\sigma_i}
      \,\phi\!\left(\frac{z}{\sigma_i}\right)
\end{equation*}
Suppose also that the conditional distribution of the $z$-statistic, given the SNR, is normal with mean SNR and variance 1. Together, the marginal and conditional distributions determine the joint distribution of the $z$-statistic and the SNR. Therefore, we also have the conditional distribution of the SNR given the $z$-statistic. It is a mixture of normal distributions with weights $q_i(z)$, means $\mu_i(z)$, standard deviations $\tau_i$ given by

\begin{equation}q_i(z) = \frac{p_i \phi(z/\sigma_i)/\sigma_i }{f(z \mid p,\sigma)}, \quad \mu_i(z)=\frac{\sigma_i^2 - 1}{\sigma_i^2}\,z \quad\text{and}\quad \tau_i=\frac{\sqrt{\sigma_i^2 - 1}}{\sigma_i}.\end{equation}

\noindent
The conditional probability of the correct sign is
\begin{equation}
\Pr(z * \text{SNR} > 0 \mid |z|) = \sum_{i=1}^{k} q_i(z) \Phi\left( \frac{\mu_i(|z|)}{\tau_i}\right).
\end{equation}

If $z_\text{repl}$ is the $z$-statistic of an exact replication study, then the conditional distribution of $z_\text{repl}$ given the $z$-statistic of the original study is a mixture of normal distributions with weights $q_i(z)$, means $\mu_i(z)$ and variances $\tau_i^2 +1$. The conditional probability of ``successful replication'' is
\begin{equation}
\Pr(z_\text {repl} * z > 0 \text{ and } |z_\text{repl}| > 1.96 \mid |z|)  = \sum_{i=1}^{k} q_i(z) \Phi\left( \frac{\mu_i(|z|) - 1.96}{\sqrt{\tau_i^2+1}}\right). 
\end{equation}

\begin{figure}
\centering
\includegraphics[width=16.5cm]{fig/gap_plot_all.pdf}
\caption{Relationship between absolute $z$-statistics and the probability that the direction of the observed effect matches the true direction (blue curves) and the probability of ``successful replication'' (red curves).}\label{fig:sign_and_repl}
\end{figure}

\printbibliography[title=References]

\end{document}
