\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,url,epsfig,float,xcolor}
%%% Document layout, margins
\usepackage{geometry} 
\geometry{letterpaper, textwidth=6.5in, textheight=9in, marginparsep=1em}
%%% Section headings
\usepackage{sectsty} 
\usepackage[normalem]{ulem}
%\setlength{\baselineskip}{40pt}
%\usepackage{pdfpages}

\sectionfont{\sffamily\bfseries\upshape\large}
\subsectionfont{\sffamily\bfseries\upshape\normalsize} 
\subsubsectionfont{\sffamily\mdseries\upshape\normalsize}
\makeatletter
\renewcommand\@seccntformat[1]{\csname the#1\endcsname.\quad}
\makeatother\renewcommand{\bibitem}{\vskip 2pt\par\hangindent\parindent\hskip-\parindent}
\newcommand{\mme}{\mathbb{E}}

\makeatletter
\def\@maketitle{%
  \begin{center}%
  \let \footnote \thanks
    {\large \@title \par}%
    {\normalsize
      \begin{tabular}[t]{c}%
        \@author
      \end{tabular}\par}%
    {\small \@date}%
  \end{center}%
}
\makeatother


\usepackage{mathtools}
\usepackage{extarrows}
\usepackage{enumerate} 
\usepackage{hyperref}
\usepackage{bbm}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=black,
}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage[toc,page]{appendix}
\newcommand{\note}[1]{{\color{red}#1}}
 \usepackage{comment}
 \usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{microtype}
 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{relsize}
\let\oldv\verbatim
\let\oldendv\endverbatim
\def\verbatim{\par\setbox0\vbox\bgroup\oldv}
\def\endverbatim{\oldendv\egroup\fboxsep0pt \noindent\colorbox[gray]{0.96}{\usebox0}\par}
%%%%%
 % \startlocaldefs

 % \endlocaldefs

\DeclareMathOperator{\SNR}{\mathrm{SNR}}
\DeclareMathOperator{\normal}{\mathrm{normal}}
\DeclareMathOperator{\MVN}{\mathrm{MVN}}
\DeclareMathOperator{\uniform}{\mathrm{uniform}}
\DeclareMathOperator{\Poisson}{\mathrm{Poisson}}
\DeclareMathOperator{\exponential}{\mathrm{exponential}}
\DeclareMathOperator{\binomial}{\mathrm{binomial}}
\DeclareMathOperator{\Cauchy}{\mathrm{Cauchy}}
\DeclareMathOperator{\logit}{\mathrm{logit}}
\DeclareMathOperator{\logistic}{\mathrm{logistic}}
\DeclareMathOperator{\lognormal}{\mathrm{lognormal}}
\DeclareMathOperator{\E}{\mathrm{E}}
\DeclareMathOperator{\Var}{\mathrm{var}}
\DeclareMathOperator{\sd}{\mathrm{sd}}
\DeclareMathOperator{\Cov}{\mathrm{cov}}
\DeclareMathOperator{\Dev}{\mathrm{Dev}}
\DeclareMathOperator{\Beta}{\mathrm{beta}}
\DeclareMathOperator{\mode}{\mathrm{mode}}
\renewcommand{\Pr}{\mathrm{Pr}}


\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{% general command to set parameter(s)
  basicstyle=\small\ttfamily
  }


\usepackage[style=authoryear-comp,indexing=cite]{biblatex}
\ExecuteBibliographyOptions{sortlocale=general,maxnames=12,maxcitenames=3,parentracker=true,sortcites=false,uniquename=false,dashed=false,hyperref=true}
\uspunctuation
\addbibresource{references.bib}
\renewbibmacro{in:}{}


\begin{document}\sloppy

\title{\bf A Statistical Case for Qualified Scientific Optimism\vspace{.1in}}

\author{
Erik van Zwet\thanks{Department of Biomedical Data Sciences, Leiden University Medical Center.}
\and Andrew Gelman\thanks{Department of Statistics and Department of Political Science, Columbia University, New York.}
\and Witold Wi\k{e}cek\thanks{Development Innovation Lab, University of Chicago.}
\vspace{.1in}
}

\date{12 Feb 2026}

\maketitle

\begin{abstract}

We create an open-source database from about 80,000 studies, combining 11~publicly available curated collections of empirical research in medicine, economics, psychology, psychotherapy, ecology, evolution, political science, education, and more, and make it available at \url{https://github.com/wwiecek/BEAR}. Each result is represented by a $z$-value: an estimated effect divided by its standard error.

We then provide a modeling framework that allows us to move away from the usual attempts to classify effects as exactly zero or not, and instead focus on identifying the direction of effects and their replicability. 

After adjusting for publication bias, we find two patterns that hold across all datasets and their various subsets: (1) statistical significance does not imply a high probability of successful replication, but (2) it does imply a high probability that the direction of the observed effect was correct. 

We conclude that replication rate is a poor measure of the quality of a field of research and argue that an attitude of global scientific pessimism based on low replication rates is misguided. Instead, we favor a more optimistic view of scientific research based on the ability to identify the direction of effects. This optimism is not naive about the many challenges science continues to face such as questionable research practices (QRPs), paper mills, fake AI papers and such. 

\end{abstract}

\section{Studying the quality of empirical science}\label{introduction}

The replication crisis has made us aware that there's a wide range of quality in the life and behavioral sciences, even considering subsets such as particular research areas and papers published in top journals.  This makes sense; if we roughly measure the quality of a study by its signal-to-noise ratio (the true effect size divided by its standard error), then we have some control over the denominator (by taking more or better measurements) but less control over the numerator. It's the nature of research to be uncertain and it's the duty of researchers to pursue all avenues, studying effects which might turn out to be null or highly context-dependent.

A key turning point in the replication crisis was the recognition and refutation of the false intuition that you can tell whether a study is good or not (tell whether a result is strong or not) using a statistical significance threshold.  Beyond the problems with any sharp threshold, even seemingly clear distinctions such as, for example, $p < 0.01$ compared to $p > 0.2$ do not discriminate well between high and low signal-to-noise ratios,\footnote{The two-sided $p$-values of 0.01 and 0.2 correspond to absolute $z$-values of 2.58 and 1.28, respectively. These are about equally likely when the signal-to-noise ratio is $(2.58+1.28)/2=1.93$. This signal-to-noise ratio corresponds to a little under 50\% power as $\Pr(|z| \geq 1.96 \mid \text{SNR}=1.93) = 0.49$.} and a similar problem arises not just with $p$-values but also with Bayes factors and other measures of quality of evidence. 

The message of the present paper is that the difficulty of assessing the signal-to-noise ratio for any given study should not be taken as a negative verdict on statistically-based science as a whole.

To study this, we have created an extensive open-source database of empirical research results, composed of 15 publicly available datasets of studies in diverse fields.  Our optimistic finding is that, in these corpora, most results seem to be correct in sign.  Our remaining pessimism is that any single study is limited in what it can teach us, which points toward the need for real replication---not in the hope of confirming significance but to understand effects, and to minimize selection in all phases of the process.

\subsection{Going beyond the characterization of scientific claims as true or false}

In an influential paper, \textcite{ioannidis2005most} modeled empirical research as ``a $2 \times 2$ table in which research findings are compared against the gold standard of true relationships in a scientific field.''
%Assuming statistical tests are done at significance level $\alpha=0.05$ (two-sided), Ioannidis characterizes a research field in terms of the ratio of the number of ``true relationships'' to ``no relationships'' $R$ and the power, $1 - \beta$, the probability of obtaining a statistically significant result conditional on there being a true relationship. He proceeds to derive expressions for the $2 \times 2$ table of proportions of true and false positive and negative findings, which is also sometimes called the confusion matrix. Among these proportions, he emphasizes the  positive predictive value (PPV), the post-study probability of a research finding being true, conditional on its estimate achieving formal statistical significance. He finds $\mathit{PPV} = (1 - \beta)R/(R - \beta R + 0.05)$ and concludes that a research finding is more likely true than false if $(1 - \beta)R > 0.05$.

This $2 \times 2$ model is implicitly accepted in the meta-scientific literature, which tends to focus on statistical significance and replication rates \parencite{simmonsFalsePositivePsychologyUndisclosed2011, opensciencecollaborationEstimatingReproducibilityPsychological2015a, camerer2016evaluating}. In particular, low replication rates are taken by many as evidence of published empirical research being full of null claims and hence not credible as a whole, a position with political influence; see, for example, \textcite{garisto2025trump}.

We understand that the $2 \times 2$ model was always intended as an instructive simplification, introduced for the purpose of explaining that in an area of research where the power is low and there are few true relationships, many statistically significant results may not be true. However, far-reaching conclusions from this model have been made about the actual state of scientific research, in particular the claim that more than half of all published research findings are false.

We have two main criticisms of the $2 \times 2$ model. First, we disagree with the notion that the primary goal of scientific study is to separate ``no relationships'' or nulls from ``true relationships.''  Effects of interest are rarely exactly zero, and even if they are, they will no longer be zero if one takes leakage, expectation effects, dropout, etc. into account. But apart from that, with finite data, it is simply not possible to distinguish effects that are zero from those that are small.

Our second criticism is that the $2 \times 2$ model does not comport with data we have collected on published research. We have compiled a database of 15 corpora, many containing thousands of studies from medicine, economics, psychology, psychotherapy, ecology, evolution, political science, education, and more. We have used these data to build more realistic models of research areas.

% Our second criticism is that the $2 \times 2$ model is not empirical, i.e. it is not based on data---and because we cannot statistically distinguish small and zero effects, it could not be well-identified from data anyway.
% %WW: I think currently we say what we do twice ("we have compiled" and then "contribution is two-fold"), so I tried to reorganise and add this bit of context:
% And while in the broader field of metascience some excellent quantitative work has been done since \parencite{opensciencecollaborationEstimatingReproducibilityPsychological2015a}, the broader understanding of quality of empirical research is fragmentary and based on small samples of studies.

In this paper we make two contributions. First, we make a large number of results from empirical research easily accessible in a new database, Benchmarks of Empirical Accuracy in Research (BEAR). We do not contribute any new data, but we process the available data to make them ready for meta-scientific research. We provide clear documentation and all data are available under open source license at \url{https://github.com/wwiecek/BEAR}. We will continue to add new datasets as they become available.

Second, we extend the meta-scientific model of \textcite{stephens2017false,van2021statistical,zwet2022proposal}  to account for truncation and publication bias. This model allows us to study measures of research quality that are insensitive to whether effects are exactly zero or merely very small, such as the probability of correctly identifying the direction of an effect and the probability of replication \parencite{greenland2017invited, gelman2014beyond, gelman2000type}. 

\subsection{Summary of findings}

We estimate from our analysis that statistical significance usually implies a high probability that the direction of the observed effect is correct, but that it does not imply a high probability that an exact replication study will reach statistical significance. In other words, it is expected that true findings (in the sense of correctly identifying the direction of an effect) will often not replicate. Similar conclusions were reached by \textcite{bak2022revisiting} and \textcite{neves2022most} on the basis of simulation models.

The scientific process is far from perfect, and questionable research practices, paper mills, and outright fraud are serious concerns \parencite{aquarius2025tackling,matusz2025threat}. But the mere fact that many studies have low probability of reaching statistical significance and therefore fail to replicate does not imply that most published research findings are false. 

The challenge is internalizing the juxtaposition of two ideas.  On one hand, there really is a replication crisis:  substantive and statistical theory, empirical evidence, and sociological reasoning all point toward the conclusion that traditional procedures for scientific quality control---peer review, causal identification, and statistical significance---are not enough to ensure replicability or to bring us closer to an understanding of reality.  On the other hand, the statistical evidence implies that in many fields, published results that reach $p<0.05$ generally fall in the correct direction.

%Evidence for the replication crisis:
%\begin{itemize}
%\item Substantive theory:  Many heavily-promoted claims (extra-sensory perception, power pose, mind-body healing, shark attacks, etc.) defy common sense.  This is not to say that they \emph{can't} be true, just that there is legitimate skepticism about them. Also, as discussed in the piranha paper, these claims can't all coexist.
%\item Statistics:  Researcher degrees of freedom, forking paths, type M and S errors, reasons why we can't take statistically significant findings at face value.
%\item Empirical evidence:  Various individual studies that did not replicate (elderly walking, etc.), also large-scale replication studies.
%\item Sociological reasoning:  Incentives for big claims, the feedback loop of overestimates and overconfidence.
%\end{itemize}
%A key takeaway from the replication crisis that we shouldn't automatically trust published results, even if they come from a respected institution and are published in a top journal.


\section{Data}

We have compiled a large number of empirical results from a variety of sources in a new database which we call Benchmarks of Empirical Accuracy in Research (BEAR). The online version of BEAR includes 20 datasets and 11.5 million $z$-values from empirical research, but for this paper we focus on $z$-values found in about 80,000 studies across 11~curated datasets. For comparison, we also show two sets of replications and two large-scale reviews which scraped $p$-values from abstracts. Characteristics of the presented datasets are given in Table~\ref{tab:dataset_summary}. We will refer to the datasets by their names from that table. 

Among the 11~curated, domain-specific collections that are our focus here, we include: 
a large collection of mainly clinical trials which posted results on \texttt{clinicaltrials.gov} or the EU Clinical Trials Repository \parencite{clinicaltrials2025,euctr2025}, 
a subset of 30,000 trials from the Cochrane Database of Systematic Reviews (CDSR; \parencite{schwab2024cochrane,cochrane2025cdsr}), which is well-curated and large (over 400,000 data rows), and 13,000 studies in ecology and evolution \parencite{costello2022decline,yang2024large}. 

Other datasets (each numbering from hundreds to about 2,000 studies) are in
education research, compiled by What Works Clearinghouse \parencite{wwc2025studyfindings}; 
psychotherapy \parencite{cuijpers2024effects}, 
psychology \parencite{rodriguez2022psymetadata},
cognitive impacts of exercise \parencite{bartos2025exercise,singh2025exercise},
intelligence research \parencite{nuijten2020intelligence},
economics \parencite{askarov2023significance, brodeur2024preregistration}, 
and political science \parencite{arel2022quantitative}. 

Six of the datasets are derived from curated databases used mainly for aggregation and evaluation of interventions. The rest are usually metascientific investigations, for example of pre-registration, publication bias, statistical power. Some datasets are themselves sets of meta-analyses, but in this paper we treat them as collections of individual studies. However, the meta-analytic structure is retained in BEAR and may be useful for other metascientific research, for example characterizing heterogeneity in treatment effects \parencite{single_meta}. 

For comparison, we also include two replication projects in psychology \parencite{opensciencecollaborationEstimatingReproducibilityPsychological2015a, klein2018manylabs} and two large-scale data-scraping papers which used abstracts or full texts of biomedical literature available via Medline and PubMed: \textcite{chavalarias2016evolution} provide 8 million $p$-values across 1.8 million studies, while \textcite{barnett2019examination} collect effect estimates of binary outcomes (odds ratios, risk ratios and such) and their confidence intervals in over 400,000 studies. 

\input{table1.tex}

While we do some data processing of our own, all of the data collection and the vast majority of data processing was already done by the creators of the datasets we reference. Most of them already include $z$-values or effect sizes and standard errors, but in some cases we make calculations ourselves (for example, from $p$-values or counts for binary data); calculation details are in \textit{Appendix: Datasets included in BEAR}. The average number of estimates per study for each dataset is given in Table \ref{tab:dataset_summary}; it can be as high as about 48 $z$-values per study in the case of \textcite{brodeur2024preregistration}, but in most cases we have either one or several estimates per study.

% Our consolidated database has, for each published estimate, the data source, $z$-value, $p$-value, effect size estimate, standard error, sample sizes, year of publication, study type (e.g., randomized trial, observational study), measure type (e.g., difference of means, log risk ratio), and some categorization into fields of study and subgroups (e.g., medical field, clinical study phase). The current version of BEAR is a single table with xx rows. 

% In some cases we have access to summary data from which we can calculate effect estimates ourselves, but in most datasets we do not know the underlying method of calculation. Where only $p$-values are available, we assume they are two-sided and calculate the $z$-value as $z = \Phi^{-1}(1 - p/2)$. In four of the datasets, the $p$-values or $z$-values are truncated if researchers reported only $p < 0.05$, $p < 0.001$, etc. We account for this in our statistical model. 

\section{Statistical model}

We extend the model of \textcite{stephens2017false,van2021statistical,zwet2022proposal} to account for the truncation and publication bias which are conspicuously present in some of our datasets. The main goal is to estimate a latent distribution of the signal-to-noise ratios across a field of research and then use this distribution to derive various aspects of the sampling distribution of replication results.

\subsection{Signal-to-noise ratios}
 
Suppose that we have a collection of unbiased effect estimates which are normally distributed with known standard errors. The $z$-value is the ratio of the effect estimate to its standard error. We define the signal-to-noise ratio (SNR) as the ratio of the (unobserved) true effect to the standard error of its estimate. Our assumptions imply that the $z$-value is equal to the SNR plus an independent standard normal error. 

The null hypothesis that there is no effect is equivalent to the hypothesis that the SNR is zero. This hypothesis is commonly rejected when the absolute value of the $z$-value exceeds 1.96 or, equivalently, when the associated two-sided $p$-value is less than 0.05. The type I error probability of this test is $\alpha=0.05$. In our notation, 
$$\Pr(|z| \geq 1.96 \mid \text{SNR}=0)=0.05.$$ 

The probability of statistical significance (PoS) depends on the true effect and the standard error through the magnitude of the SNR,

\begin{equation}
\text{PoS}(\text{SNR})=\Pr(|z| \geq 1.96 \mid \text{SNR}) = \Phi(-1.96 - |\text{SNR}|) + 1 - \Phi(1.96 - |\text{SNR}|). \label{eq:power}
\end{equation}
For example, if the $\text{SNR}=2.8$ (or $-2.8$), then the PoS is 80\%. Averaging the PoS over the distribution of the SNR in a field of research, we obtain the ``assurance''  which we denote by $\overline{\text{PoS}}$. It is the proportion of studies for that field that reaches the 5\% level of statistical significance.  We can also consider  $\text{Pr}(|\text{SNR}| \geq 2.8) = \text{Pr}(\text{PoS(SNR)} \geq 0.8)$, which is the proportion of studies for which the power relative to the true effect size is 80\% or more.

The inherent variability of $p$-values can be most easily understood by considering SNRs and $z$-values. Suppose a study has 80\% power. In that case, the sampling distribution of the $z$-value is normal with mean 2.8 and unit variance. So, both $z=0.8 (p=0.42)$ and $z=4.8 (p=1.6 \cdot 10^{-6})$ can easily occur. 

\subsection{A model for the distribution of signal-to-noise ratios in a corpus}

We will use maximum likelihood to estimate the distribution of SNRs given a sample of absolute $z$-values. Our assumptions imply that $z$-values are equal to SNRs plus independent standard normal errors. We can therefore obtain the distribution of the absolute SNRs by deconvolution of the standard normal error component from the distribution of the absolute $z$-values \parencite{efron2016empirical,stephens2017false}. 

We construct the likelihood in four stages. First, we introduce a mixture distribution for the absolute $z$-values. Then, we add a parameter to account for publication bias. Next, we account for the fact that reported values are sometimes truncated. Finally, we weight the observations according to the number of $z$-values per study.

Our main assumption is that the distribution of the absolute $z$-values is well represented by a mixture of half-normal distributions. We will informally verify this assumption by inspection of the observed histograms. Noting that this assumption is equivalent to modeling the signed $z$-values as a mixture of zero-mean normal distributions, we have the mixture density,
\begin{equation}
f(z \mid p,\sigma) = \sum_{i=1}^{k} p_i\,\frac{1}{\sigma_i}
      \,\phi\!\left(\frac{z}{\sigma_i}\right),                  \label{eq:mixture}
\end{equation}
where $\phi(\cdot)$ is the standard normal density,   $p=(p_1,\dots,p_k)$ are the non-negative mixture weights that sum to unity and  $\sigma=(\sigma_1,\dots,\sigma_k)$ are the standard deviations. These standard deviations are at least 1 because we know that $z$-values have a standard normal noise component. We will use $k=4$, as we have verified that larger values of $k$ yield nearly identical results. 

The deconvolution to obtain the distribution of the SNRs is easy. We simply subtract 1 from the variances of the mixture components. So,
\begin{equation}
f_\text{SNR}(x \mid p,\sigma) = f(x \mid p,\sqrt{\sigma^2 -1}).
\end{equation}

To account for publication bias, we introduce a simple selection component in the likelihood. We follow \textcite{hedges1984estimation,hedges1992modeling} and define $\omega$ as the relative risk that a result with $|z|<1.96$ is observed compared to a result with $|z| \geq 1.96$
\begin{equation}
\omega \;=\;
\frac{\Pr(\text{result is published} \mid |z| <1.96)}
     {\Pr(\text{result is published} \mid |z| \ge1.96)}.
\end{equation}
\noindent
The probability of publication is
\begin{equation}
C(p,\sigma,\omega) = \omega\,\Pr(|z| < 1.96 \mid p,\sigma) + \Pr(|z| \geq 1.96 \mid p,\sigma)
\end{equation}
and the density of the $z$-values conditional on publication becomes

\begin{equation}
f(z \mid p,\sigma, \omega, \text{pub}) = 
\begin{cases}
\omega\,f(z \mid p,\sigma) / C(p,\sigma,\omega) \quad   & |z| \leq 1.96 \\
      f(z \mid p,\sigma)/ C(p,\sigma,\omega)                              & \text{otherwise.}
\end{cases}
\end{equation}

The $z$-value is sometimes left or right truncated, most often at 1.96, but also at, for example, 2.57 (when papers report $p < 0.01$), 3.29 ($p < 0.001$), or 1.64 ($p < 0.1$). To take this into account, we introduce a censoring indicator $\delta$ taking the values $<$, $>$, or $=$ for left censoring, right censoring, or no censoring, respectively. We denote the (possibly) censored value of the $z$-value by $\tilde{z}$. The distribution of the pair $(\delta,\tilde{z})$ conditional on publication is
\begin{align*}
&f(\tilde{z},\delta \mid p,\sigma, \omega, \text{pub})= \\
&\begin{cases}
\omega\,f(\tilde{z}\mid p,\sigma)/ C(p,\sigma,\omega)         & \delta=\texttt{"="},\; \tilde{z}<1.96,\\[4pt]
      f(\tilde{z}\mid p,\sigma)/C(p,\sigma,\omega)              & \delta=\texttt{"="},\; \tilde{z}\ge 1.96,\\[6pt]
\omega\,F(\tilde{z}\mid p,\sigma)/ C(p,\sigma,\omega)            & \delta=\texttt{"<"},\; \tilde{z}< 1.96,\\[4pt]
(\omega\,F(1.96\mid p,\sigma)+F(\tilde{z}\mid p,\sigma)-F(1.96\mid p,\sigma))/ C(p,\sigma,\omega)          & \delta=\texttt{"<"},\; \tilde{z}\ge  1.96,\\[6pt]
(1-F(\tilde{z}\mid p,\sigma))/ C(p,\sigma,\omega)          & \delta=\texttt{">"},\; \tilde{z}\ge 1.96,\\[4pt]
((1-F(1.96\mid p,\sigma))+\omega\,(F(1.96\mid p,\sigma)-F(\tilde{z}\mid p,\sigma)))/ C(p,\sigma,\omega)
                                               & \delta=\texttt{">"},\; \tilde{z}<  1.96.
\end{cases}
\end{align*}
\noindent
The first two rows cover the case without truncation. Rows three and four are left censoring (statements such as $p > 0.05$), and  rows five and six deal with right censoring (e.g., $p < 0.001$). We also introduce artificial censoring when $z$-values are reported to be exactly zero, by treating them as $|z| < 0.5$.

Most of the datasets have multiple $z$-values per study. To prevent studies with many $z$-values from dominating the likelihood, we weight observations by $w_j$, the inverse of the number of $z$-values originating from study $j$. This does not account for the dependence among $z$-values from the same study but that does not concern us because we do not attempt to quantify the uncertainty of our estimates.  The reason is that our datasets are so large that the sampling uncertainty which determines standard errors and confidence intervals, is negligible compared to model uncertainty. The full weighted log likelihood is  
\begin{equation}
\ell(p,\sigma, \omega)
= \sum_{j=1}^{n} w_j \log f(\tilde{z_j},\delta_j \mid p,\sigma, \omega, \text{pub}).               \label{eq:loglik}
\end{equation}
\noindent
Even for large datasets, numerical optimization of this log likelihood is feasible, taking just minutes.  Large datasets with hundreds of thousands or even millions of observations could be randomly subsampled to, say, 50,000 observations---but we would see no reason to do this, because if such a large corpus were available, it would make sense to break it up and estimate the distribution of SNRs separately for each sub-corpus.

The estimated distribution of the $z$-values without publication bias is $f(z \mid \hat p,\hat \sigma)$ as defined in (\ref{eq:mixture}) and consequently the estimated distribution of the SNRs is
\begin{equation}
f_\text{SNR}(x \mid \hat p,\hat \sigma) = f(x \mid \hat p,\sqrt{\hat \sigma^2 -1}).
\end{equation}

We can derive many important quantities from the distribution of the SNRs. For example, the PoS is a function of the SNR, so we can obtain its distribution by transforming the distribution of the SNRs. In particular, the proportion of studies that have at least 80\% probability of reaching statistical significance is
\begin{equation}
\Pr(\text{PoS} \geq 0.8) = \Pr(|\text{SNR}| \geq 2.8).
\end{equation}
The marginal density of the SNRs, together with the conditional density of the $z$-values given the SNRs, determines the joint distribution. And from the joint  distribution we can derive the conditional distribution of the SNR given the $z$-value.
We provide the formulas in Appendix A. In turn, this allows us to derive the conditional probability of the correct sign,
\begin{equation}\label{eq:sign}
\text{Probability of the correct sign} = \Pr(z \cdot \text{SNR} > 0 \,\big|\, |z|),
\end{equation}
Finally, we imagine that a second replication study with exactly the same SNR yields another $z$-value $z_\text{repl}$. This  $z_\text{repl}$ is conditionally independent of the original $z$ given SNR, and has the same distribution. Thus \textcite{vanzwet2022large} were able to derive the conditional distribution of $|z_\text{repl}|$ given $|z|$. In particular, we obtain 
\begin{equation}\label{eq:repl}
\text{Probability of ``successful replication''} = \Pr(z_\text{repl} \cdot z > 0 \text{ and } |z_\text{repl}| > 1.96 \,\big|\, |z|) 
\end{equation}

All the computations that yielded the results reported in the next section are documented in the online supplement to this paper. 

\section{Results}

Figure~\ref{fig:zmixtures} displays the histograms of the observed absolute $z$-statistics of our 15 datasets. The solid curves are the fitted mixtures of half-normal distributions conditional on publication; dashed curves are the unconditional distributions. We find that the estimated distributions generally track the histogram well and conclude that our model is appropriate for our purpose of quantifying large-scale properties of research areas. Our main focus is the 11 curated, domain-specific sets (sets of single studies or collections of meta-analyses), which comprise the first 3 rows, shown in blue and arranged by their estimated assurances $\overline{\text{PoS}}$. 

Mixtures of half-normal densities are necessarily decreasing. Apart from the effects of publication bias, we also see this feature in the histograms. We claim that this is typical of large collections of $z$-values. To further support this claim, we have broken down the Cochrane data by 19 medical specialties and the data from {\tt clinicaltrials.gov} by trial phase, see Figure~\ref{fig:subsets}. Again, we see decreasing distributions of absolute $z$-values.

The discontinuities (``jumps'') of the densities at $|z|=1.96$ are a feature of Hedges' selection model. It is tempting to focus on the extent of publication bias, which may in principle be quantified by the estimated relative risk of publication $\hat \omega$ as reported in Table~\ref{tab:results_summary}. However, there are several reasons why we must resist that temptation. Many studies in our datasets have multiple outcomes, and we do not know which of them drive publication decisions. Moreover, while Hedges' single-parameter selection model is useful for adjusting our estimates of the distributions of the $z$-statistics, it seems too limited for reliably quantifying and comparing the extent of publication bias across fields of research. This is also not the goal of our study.

The main difference between the distributions of the $z$-values is their widths. A wider distribution of $z$-values implies a wider distribution of SNRs. In other words, the studies in that research area tend to have larger absolute SNRs. Since the PoS is an increasing function of the absolute SNR, this also means that a higher proportion of results will reach statistical significance. Recall that the assurance $\overline{\text{PoS}}$ is the proportion of significant results, adjusted for publication bias; see (\ref{eq:power}). The assurance is less than the raw proportion of significant results, unless $\hat{\omega}=1$ (i.e., no discernible publication bias). We report the estimated assurances $\overline{\text{PoS}}$ in Figure~\ref{fig:zmixtures} and also in Table~\ref{tab:results_summary}. Among the 11 curated datasets, the estimated assurances range from 0.23 to 0.48.  Interestingly, the estimated assurance of phase 3 clinical trials in Figure~\ref{fig:subsets} matches the estimated success rates (probability of moving to regulatory approval following trial) in phase 3 trials in drug development reported by \textcite{thomas2021clinicaldevelopment}.

By transforming the distribution of the signal-to-noise ratios, we obtain the full distribution of the PoS within each dataset. The left panel of Figure~\ref{fig:pos_gap} displays the complementary cumulative distribution functions of the PoS in the 11 curated datasets. In other words, it shows what proportions of studies in each dataset ``survive'' to achieve a given level of power. We find that between 9\% and 29\% of studies achieve 80\% PoS (Table~\ref{tab:results_summary}, column 4). 

Since the great majority of studies are targeted at reaching 80\% power, the proportion of studies that reach 80\% PoS may be tentatively interpreted as the proportions of studies where the assumptions of the sample size calculation were true. In general, this proportion is low. However, sample size calculations are not supposed to be aimed at the true effect (which is never known) but at the effect one would not want to miss \parencite{senn2002cross}. In that sense, a low proportion of studies with 80\% PoS just means that it is hard to come up with interventions that have large effects. Pressures of time, money, and logistics also can drive down sample sizes.  

Columns 5--10 of Table~\ref{tab:results_summary} report the estimated proportion of studies which would yield a significant result if replicated (``successful replication''), and the estimated proportion of studies where the observed effect has the correct direction (sign), as defined in equations (\ref{eq:sign}) and (\ref{eq:repl}). There is a large gap between these proportions. Depending on the dataset, the estimated probability of replication ranges from 21\% to 47\% while the estimated probability of correct sign ranges from 69\% to 88\%. 

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{fig/mixtures_plot.pdf}
\caption{Histograms of absolute $z$-values across 15~datasets with fitted curves overlaid on top. The solid curves are the fitted mixtures of half-normal distributions conditional on publication. Dashed curves are the unconditional distributions. Blue: curated datasets; yellow: replications; green: large-scale scraped datasets.}\label{fig:zmixtures}
\end{figure}

\input{table2.tex}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{fig/pos_gap.pdf}
\vspace{-.2in}
\caption{Left panel: ``Survival'' of significance: $1 - (\mbox{CDF of PoS})$ for each of the 15 datasets. Right panel: Conditional probability that observed direction of the effect matches the true direction (top set of curves) and the conditional probability of ``successful'' replication (bottom set). Colors here are the same as in Figure~\ref{fig:zmixtures}.}\label{fig:pos_gap}
\end{figure}

If we restrict only to studies that reach statistical significance (columns ``if $\vert z \vert > 1.96$''), the difference is even more stark: replication probability in 11~curated datasets ranges from 54\% to 75\%, while the probability of correctly identifying direction of effect is 94-99\%.

The right panel of Figure~\ref{fig:pos_gap} shows the probabilities of these two quantities conditional on $|z|$. 
%, with $|z| = 1.96$ reported in Table~\ref{tab:results_summary}. When the $z$-value reaches 1.96, the probability of the correct sign is 81\% to 96\% . At the same time, the probability of ``successful replication'' is very small, 23\% to 41\% at $|z|=1.96$ and reaching 80\% or 90\% only for extremely large values of $|z|$.
If a result is marginally statistically significant at the 5\% level, the probability of ``successful replication'' is often not increased compared to the base rate. We show this more clearly in Supplementary Figure~\ref{fig:gap_plot_all} where we also plot the average replication probability and find that it tends to be higher than the conditional probability at $|z|=1.96$ (also see Table~\ref{tab:results_summary_supplement} for exact values). This will be a surprise to those who think of statistical significance as a stand-in for replication. This phenomenon is {\em not} a consequence of many studies having low SNRs (low PoS). If all the studies had higher SNRs, then the assurance would be higher, but observing $|z|=1.96$ would be even less impressive.

The bottom row of Figure~\ref{fig:zmixtures} shows the two replication datasets (yellow) and the two data scrapes (green). We find that for the replication studies the estimated relative risk of publication is equal to one, which means that---as expected---there is no evidence of publication bias among the replications. The two data scrapes, on the other hand, show signs of massive selection on statistical significance: while for the 11~curated datasets $\omega$ ranges from 0.65 to 0.95, the two scraped datasets read out $\omega=0.08$ and $\omega=0.20$. However, we must be careful not to over-interpret this result. The dataset of \textcite{barnett2019examination} is based on confidence intervals and it is possible that non-significant results were actually reported, just not with the associated confidence intervals. The noticeable spikes in the histogram of the data from \textcite{chavalarias2016evolution} are due to severe truncation. We do not consider our results based on these two datasets to be very reliable, but it does seem clear that the significance filter has left a mark on the scientific literature.

\section{Discussion}

\subsection{An alternative to the binary model of scientific discovery}

The  $2 \times 2$ model \parencite{ioannidis2005most} is often taken to be a reasonable, if simplified, description of scientific research. According to this model, the probabilities of significance (PoS) of the ``true relationships'' and ``no relationships'' are well separated. The PoS of ``no relationships'' is 5\% by the design of the statistical test, and the PoS of ``relationships'' is much larger by the design of the experiment. If the ratio of ``true relationships'' to ``no relationships'' is not too low, then we can reliably separate them. That is, the positive predictive value (the conditional probability of a true relationship given statistical significance) and the negative predictive value (the conditional probability of no relationship given non-significance) are both high. Moreover, the conditional probability of successful replication (reaching statistical significance again in an exact replication study) is also high.

We have developed an alternative statistical model that goes beyond the $2\times 2$ framework by allowing the PoS to range continuously from 0.05 to 1 and uses the Hedges' model to adjust for selective reporting. We also allow for truncation. We fit our model to corpora from about 100,000 studies across diverse fields collected from publicly available sources.

The model fits well; see Figures~\ref{fig:zmixtures} and \ref{fig:subsets}. In contrast, the $2 \times 2$ model does not fit our data, as we observe no clear separation between the probability of statistical significance for ``true relationships'' and ``no relationships.''

\subsection{What our meta-analysis says about effect sizes and replication rates}

There are four notable features that hold across all the datasets of empirical research we collected. First, the absolute $z$-values have a decreasing density, which implies that the absolute SNRs also have a decreasing density. In other words, studies with low SNRs are more common than those with high SNRs. 

However, there are differences between research domains in the widths of these distributions which means that the studies in some areas tend to have higher SNRs than in other areas. 

Second, the probability of significance (PoS) tends to be low and only a small minority of studies achieve 80\% power. 
Third, reaching statistical significance does not imply a high probability of replication: see Figure~\ref{fig:pos_gap}. Even at $p = 0.001$ replication is far from assured. 
Fourth, statistical significance {\em does} imply that the sign of the effect is likely correct. 


\subsection{Reasons for qualified optimism about empirical research}

Instead of focusing on the paradigm of ``no relationships'' versus ``true relationships'' and associated type I and type II errors, our model allows us to examine replication and direction identification, both of which are insensitive to whether effects are zero or merely small. 

We find that statistical significance usually implies a high probability that the direction of the observed effect is correct, but that it does not imply a high probability that an exact replication study will be significant again.
In other words, empirical research often leads to true findings---in the sense of correctly identifying the direction of an effect---that do not replicate. Therefore, failure to replicate should not be taken to mean that the original result was a fluke or a fake.

This should not come as a surprise, as the difference between ``significant'' and ``not significant'' is not itself statistically significant \parencite{gelman2006difference}. And yet, it can be viewed as scandalous when a statistically significant published result does not replicate \parencite{tversky1971}. This suggests a widespread discrepancy between expectations and reality which we believe to be due to the unrealistic $2 \times 2$ model that people have in mind.

The pessimism in much of the meta-scientific literature reflects a one-sided interpretation of Figure~\ref{fig:pos_gap} focusing only on the low replication rates. This skewed view has serious consequences as it may lead to dangerous nihilism among policy makers \parencite{garisto2025trump}. We argue for a more balanced view where we also take into account the high probabilities of getting the sign right.

\subsection{Understanding our findings using the anthropic principle}

The current paper is meta-scientific in that it is based on an analysis of corpora of published results. 
Why do these different corpora, coming from many different experiments in different fields, show similar distributions of the SNR? We tentatively ascribe this to a sort of anthropic principle \parencite{anthropic} whereby researchers respond to the pressures of time, money, and logistics on one side and the goal to reach statistical significance on the other, by making their studies just about large enough to assess the direction of the effect of interest, but not larger. 

%second half of this para is implications for practice?
That said, these incentives do not apply in all areas, and there are subfields of science that are plagued by questionable research practices and increasingly influenced by paper mills and fraudulent research. 
If we have good prior reasons to doubt the study design or integrity of the analysis, we should use that information. In other words, not every result should be treated as a typical draw from the broad reference class. Our qualified optimism extends only to research for which we do not have decisive reasons to distrust.


\subsection{Relevance to statistical practice}
% this now recaps a lot, could wrap it into the earlier discussion?
We conclude by comparing to standard practice, which is that if a result reaches a statistical significance threshold, it is treated as real and the corresponding estimate is taken at face value, while statistically non-significant results are presented as null findings or, at best, scenarios where there is not enough information to draw any conclusions.

Along with our qualified optimism about the direction of effects, our analysis suggests that a low observed rate of statistically significant replications should not be taken as evidence that most published claims are false.  
The bad news is that most studies have less than 80\% probability of significance (i.e., power relative to the true effect size); the good news is that, even though average probabilities of significance range from 20\% to 50\%, replications are likely to go in the right direction, with estimated average type S error rates in our corpora in the range of 1\%--3\%. 

\section{Acknowledgments}
We thank the authors of the datasets for their great effort.

% This is now leading into the follow-up paper---thinking at the level of individual studies?

%In contrast, in our corpus-informed Bayesian approach, estimates are not taken at face value---they are partially pooled toward zero---even in the case of statistical significance, thus adjusting for possible type M (magnitude) errors \parencite{van2021statistical,zwet2022proposal}.  And, as discussed in the present article, even non-statistically-significant results are not taken to be zero, as estimates are still likely to have the right sign.  We estimate the average type S error rates in our corpora in the range 10\%--30\%. 

%We need to strike a balance . . .
%
%
%It depends on the subfield! . . .
%
%
%\section*{To keep in mind}
%
%{\bf Disclaimer}: Our results apply to repeated sampling of studies (or a random sample of a study) from certain research fields. To apply them to a particular study, would require us to ignore everything about that study except the observed $z$-value.
%
%\medskip\noindent
%{\bf Main message 1}: Empirical finding: research areas are similar in the sense that they have decreasing distributions of absolute $z$-values (and hence decreasing distributions of absolute SNRs). This is an anthropogenic/anthropic phenomenon.
%
%\medskip\noindent
%{\bf Main message 2}: Individual studies can be bad or untrustworthy for many reasons. However, a replication rate of 30-40\% of a research area is no reason for mistrust. It's merely a consequence of the inherent variability of $z$-values, i.e. they're the convolution of some distribution that is mostly supported on $[-2,2]$ (the SNRs) and the standard normal.
%
%\bigskip
%The posts on statmodeling about $z$ and SNR show that there’s a lot of confusion. The main issues seem to be:
%\begin{itemize}
%\item Confusing the SNR with the effect. Even if the effect of some intervention is large, the distribution of $|\SNR|$ can still be decreasing.
% 
%\item Confusing selection on large $z$ (bad) with selection on large SNR (good).
% 
%\item Confusing the distribution of z across some field of study with the conditional distribution of z given SNR. The former is not supposed to look normal. In the absence of selection on $z$, it’s only supposed to be as smooth as N(0,1). That the distribution of $|z|$ is often decreasing is an empirical finding.
%\end{itemize}
%We’ll need to work very hard to explain these things! Maybe think of some good visuals?
%

\printbibliography[title=References]

\pagebreak
\clearpage

\appendix
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{figure}{0}
\section{Details of calculations}

Suppose the marginal distribution of the $z$-value is a mixture of zero-mean normal distributions,
\begin{equation*}
f(z \mid p,\sigma) = \sum_{i=1}^{k} p_i\,\frac{1}{\sigma_i}
      \,\phi\!\left(\frac{z}{\sigma_i}\right).
\end{equation*}
Suppose also that the conditional distribution of the $z$-value, given the SNR, is normal with mean SNR and variance 1. Together, the marginal and conditional distributions determine the joint distribution of the $z$-value and the SNR. Therefore, we also have the conditional distribution of the SNR given the $z$-value. It is a mixture of normal distributions with weights $q_i(z)$, means $\mu_i(z)$, standard deviations $\tau_i$ given by

\begin{equation}q_i(z) = \frac{p_i \phi(z/\sigma_i)/\sigma_i }{f(z \mid p,\sigma)}, \quad \mu_i(z)=\frac{\sigma_i^2 - 1}{\sigma_i^2}\,z \quad\text{and}\quad \tau_i=\frac{\sqrt{\sigma_i^2 - 1}}{\sigma_i}.\end{equation}

\noindent
The conditional probability of the correct sign is
\begin{equation}
\Pr(z * \text{SNR} > 0 \mid |z|) = \sum_{i=1}^{k} q_i(z) \Phi\left( \frac{\mu_i(|z|)}{\tau_i}\right).
\end{equation}

If $z_\text{repl}$ is the $z$-value of an exact replication study, then the conditional distribution of $z_\text{repl}$ given the $z$-value of the original study is a mixture of normal distributions with weights $q_i(z)$, means $\mu_i(z)$ and variances $\tau_i^2 +1$. The conditional probability of ``successful replication'' is
\begin{equation}
\Pr(z_\text {repl} * z > 0 \text{ and } |z_\text{repl}| > 1.96 \mid |z|)  = \sum_{i=1}^{k} q_i(z) \Phi\left( \frac{\mu_i(|z|) - 1.96}{\sqrt{\tau_i^2+1}}\right). 
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=16.5cm]{fig/gap_plot_all.pdf}
\caption{Relationship between absolute $z$-values and the probability that the direction of the observed effect matches the true direction (blue curves) and the probability of ``successful replication'' (red curves). The vertical line is $\overline{\mathrm{PoS}}$. }
\label{fig:gap_plot_all}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{fig/mixtures_plot_subsets.pdf}
    \vspace{-.2in}
    \caption{Subsets of interventional treatment studies from {\tt clinicaltrials.gov}, broken down by phase (top row) and from Cochrane database (bottom row), grouped by specialty (out of 19 included in the dataset): two with lowest $\overline{\mathrm{PoS}}$ on the left and two with the highest on the right. The peaks in the histograms at $|z| = 1.96, 2.58, 3.29$ are due to trials reporting (two-sided) $p<0.05, p<0.01, p<0.001$.}
    \label{fig:subsets}
\end{figure}

\input{table3.tex}

\clearpage
\section{Datasets included in BEAR}\label{app:datasets}
\input{datasets}

\end{document}
