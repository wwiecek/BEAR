@article{addaPhackingClinicalTrials2020,
  title = {P-Hacking in Clinical Trials and How Incentives Shape the Distribution of Results across Phases},
  author = {Adda, J{\'e}r{\^o}me and Decker, Christian and Ottaviani, Marco},
  year = {2020},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {24},
  pages = {13386--13392},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1919906117},
  urldate = {2025-03-05},
  abstract = {Clinical research should conform to high standards of ethical and scientific integrity, given that human lives are at stake. However, economic incentives can generate conflicts of interest for investigators, who may be inclined to withhold unfavorable results or even tamper with data in order to achieve desired outcomes. To shed light on the integrity of clinical trial results, this paper systematically analyzes the distribution of P values of primary outcomes for phase II and phase III drug trials reported to the ClinicalTrials.gov registry. First, we detect no bunching of results just above the classical 5\% threshold for statistical significance. Second, a density-discontinuity test reveals an upward jump at the 5\% threshold for phase III results by small industry sponsors. Third, we document a larger fraction of significant results in phase III compared to phase II. Linking trials across phases, we find that early favorable results increase the likelihood of continuing into the next phase. Once we take into account this selective continuation, we can explain almost completely the excess of significant results in phase III for trials conducted by large industry sponsors. For small industry sponsors, instead, part of the excess remains unexplained.},
  file = {C:\Users\witol\Zotero\storage\H4SAHXGG\Adda et al. - 2020 - P-hacking in clinical trials and how incentives shape the distribution of results across phases.pdf}
}

@article{arel-bundockQuantitativePoliticalScience2022,
  title = {Quantitative {{Political Science Research}} Is {{Greatly Underpowered}}},
  author = {{Arel-Bundock}, Vincent and Briggs, Ryan C. and Doucouliagos, Hristos and Mendoza Avi{\~n}a, Marco and Stanley, Tom D.},
  year = {2022},
  journal = {I4R Discussion Paper Series},
  number = {6},
  publisher = {The Institute for Replication (I4R)},
  urldate = {2025-03-05},
  abstract = {The social sciences face a replicability crisis. A key determinant of replication success is statistical power. We assess the power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles. Using generous assumptions, we find that the median analysis has about 10\% power and that only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. We also find substantial heterogeneity in tests across research areas, with some being characterized by high power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  langid = {english},
  file = {C\:\\Users\\witol\\Zotero\\storage\\AVSH745R\\Arel-Bundock et al. - 2022 - Quantitative Political Science Research is Greatly Underpowered.pdf;C\:\\Users\\witol\\Zotero\\storage\\VYFBFQPW\\6.html}
}

@article{askarovSignificanceDataSharingPolicy2023,
  title = {The {{Significance}} of {{Data-Sharing Policy}}},
  author = {Askarov, Zohid and Doucouliagos, Anthony and Doucouliagos, Hristos and Stanley, T D},
  year = {2023},
  month = jun,
  journal = {Journal of the European Economic Association},
  volume = {21},
  number = {3},
  pages = {1191--1226},
  issn = {1542-4766},
  doi = {10.1093/jeea/jvac053},
  urldate = {2025-03-05},
  abstract = {We assess the impact of mandating data-sharing in economics journals on two dimensions of research credibility: statistical significance and excess statistical significance (ESS). ESS is a necessary condition for publication selection bias. Quasi-experimental difference-in-differences analysis of 20,121 estimates published in 24 general interest and leading field journals shows that data-sharing policies have reduced reported statistical significance and the associated t-values. The magnitude of this reduction is large and of practical significance. We also find suggestive evidence that mandatory data-sharing reduces ESS and hence decreases publication bias.},
  file = {C:\Users\witol\Zotero\storage\UW9X3C2I\6706852.html}
}

@article{barnettExaminationCIsHealth2019,
  title = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019: An Observational Study},
  shorttitle = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019},
  author = {Barnett, Adrian Gerard and Wren, Jonathan D.},
  year = {2019},
  month = nov,
  journal = {BMJ open},
  volume = {9},
  number = {11},
  pages = {e032506},
  issn = {2044-6055},
  doi = {10.1136/bmjopen-2019-032506},
  abstract = {OBJECTIVES: Previous research has shown clear biases in the distribution of published p values, with an excess below the 0.05 threshold due to a combination of p-hacking and publication bias. We aimed to examine the bias for statistical significance using published confidence intervals. DESIGN: Observational study. SETTING: Papers published in Medline since 1976. PARTICIPANTS: Over 968\,000 confidence intervals extracted from abstracts and over 350\,000 intervals extracted from the full-text. OUTCOME MEASURES: Cumulative distributions of lower and upper confidence interval limits for ratio estimates. RESULTS: We found an excess of statistically significant results with a glut of lower intervals just above one and upper intervals just below 1. These excesses have not improved in recent years. The excesses did not appear in a set of over 100\,000 confidence intervals that were not subject to p-hacking or publication bias. CONCLUSIONS: The huge excesses of published confidence intervals that are just below the statistically significant threshold are not statistically plausible. Large improvements in research practice are needed to provide more results that better reflect the truth.},
  langid = {english},
  pmcid = {PMC6887056},
  pmid = {31753893},
  keywords = {Bibliometrics,confidence intervals,Confidence Intervals,Data Interpretation Statistical,MEDLINE,p-hacking,p-values,Probability,publication bias,Publication Bias,statistical significance},
  file = {C:\Users\witol\Zotero\storage\VJX3IMK5\Barnett and Wren - 2019 - Examination of CIs in health and medical journals from 1976 to 2019 an observational study.pdf}
}

@misc{brodeurPreRegistrationPreAnalysisPlans2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Do {{Pre-Registration}} and {{Pre-Analysis Plans Reduce}} p-{{Hacking}} and {{Publication Bias}}?},
  author = {Brodeur, Abel and Cook, Nikolai and Hartley, Jonathan and Heyes, Anthony},
  year = {2022},
  month = aug,
  number = {4180594},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4180594},
  urldate = {2022-09-07},
  abstract = {Randomized controlled trials (RCTs) are increasingly prominent in economics, with pre-registration and pre-analysis plans (PAPs) promoted as important in ensuring the credibility of findings. We investigate whether these tools reduce the extent of p-hacking and publication bias by collecting and studying the universe of test statistics, 15,992 in total, from RCTs published in 15 leading economics journals from 2018 through 2021. In our primary analysis, we find no meaningful difference in the distribution of test statistics from pre-registered studies, compared to their non-pre-registered counterparts. However, pre-registered studies that have a complete PAP are significantly less p-hacked. These results point to the importance of PAPs, rather than pre-registration in itself, in ensuring credibility.},
  langid = {english},
  keywords = {p-Hacking,Pre-analysis Plan,Pre-registration,Publication Bias,Research Credibility},
  file = {C\:\\Users\\witol\\Zotero\\storage\\PFDU4H8L\\Brodeur et al. - 2022 - Do Pre-Registration and Pre-Analysis Plans Reduce .pdf;C\:\\Users\\witol\\Zotero\\storage\\I8KYX62G\\papers.html}
}

@article{costelloDeclineEffectsAre2022,
  title = {Decline Effects Are Rare in Ecology},
  author = {Costello, Laura and Fox, Jeremy W.},
  year = {2022},
  journal = {Ecology},
  volume = {103},
  number = {6},
  pages = {e3680},
  issn = {1939-9170},
  doi = {10.1002/ecy.3680},
  urldate = {2025-03-05},
  abstract = {The scientific evidence base on any given topic changes over time as more studies are published. Currently, there is widespread concern about nonrandom, directional changes over time in the scientific evidence base associated with many topics. In particular, if studies finding large effects (e.g., large differences between treatment and control means) tend to get published quickly, while small effects tend to get published slowly, the net result will be a decrease over time in the estimated magnitude of the mean effect size, known as a ``decline effect.'' If decline effects are common, then the published scientific literature will provide a biased and misleading guide to management decisions, and to the allocation of future research effort. We compiled data from 466 meta-analyses in ecology to look for evidence of decline effects. We found that decline effects are rare. Only 5\% of ecological meta-analyses truly exhibit a directional change in mean effect size over time arising for some reason other than random chance, usually but not always in the direction of decline. Most apparent directional changes in mean effect size over time are attributable to regression to the mean, consistent with primary studies being published in random order with respect to the effect sizes they report. Our results are good news: decline effects are the exception to the rule in ecology. Identifying and rectifying rare cases of true decline effects remains an important task, but ecologists should not overgeneralize from anecdotal reports of decline effects.},
  copyright = {{\copyright} 2022 The Ecological Society of America.},
  langid = {english},
  keywords = {decline effect,effect size,hierarchical mixed effects model,meta-analysis,meta-meta-analysis,publication bias},
  file = {C:\Users\witol\Zotero\storage\KHLQDNXX\ecy.html}
}

@article{jagerEstimateSciencewiseFalse2014,
  title = {An Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature},
  author = {Jager, Leah R. and Leek, Jeffrey T.},
  year = {2014},
  month = jan,
  journal = {Biostatistics (Oxford, England)},
  volume = {15},
  number = {1},
  pages = {1--12},
  issn = {1468-4357},
  doi = {10.1093/biostatistics/kxt007},
  abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported \$P\$-values as the data. We then collect \$P\$-values from the abstracts of all 77\,430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported \$P\$-values. We estimate that the overall rate of false discoveries among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5\% more false positives (FP) per year, \$P = 0.18\$) or with respect to journal submissions (0.5\% more FP per 100 submissions, \$P = 0.12\$). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
  langid = {english},
  pmid = {24068246},
  keywords = {Algorithms,Biomedical Research,Computer Simulation,Data Interpretation Statistical,False discovery rate,False Positive Reactions,Genomics,Humans,Meta-analysis,Multiple testing,Publications,Science-wise false discovery rate,Software,Two-group model,United Kingdom,United States}
}

@misc{MetapsyMetaAnalyticPsychotherapy,
  title = {Metapsy {\textbar} {{Meta-Analytic Psychotherapy Databases}}},
  urldate = {2025-03-05},
  howpublished = {https://www.metapsy.org/},
  file = {C:\Users\witol\Zotero\storage\HPYCZSPJ\www.metapsy.org.html}
}

@article{sladekovaEstimatingChangeMetaanalytic2023,
  title = {Estimating the Change in Meta-Analytic Effect Size Estimates after the Application of Publication Bias Adjustment Methods},
  author = {Sladekova, Martina and Webb, Lois E. A. and Field, Andy P.},
  year = {2023},
  month = jun,
  journal = {Psychological Methods},
  volume = {28},
  number = {3},
  pages = {664--686},
  issn = {1939-1463},
  doi = {10.1037/met0000470},
  abstract = {Publication bias poses a challenge for accurately synthesizing research findings using meta-analysis. A number of statistical methods have been developed to combat this problem by adjusting the meta-analytic estimates. Previous studies tended to apply these methods without regard to optimal conditions for each method's performance. The present study sought to estimate the typical effect size attenuation of these methods when they are applied to real meta-analytic data sets that match the conditions under which each method is known to remain relatively unbiased (such as sample size, level of heterogeneity, population effect size, and the level of publication bias). Four-hundred and 33 data sets from 90 articles published in psychology journals were reanalyzed using a selection of publication bias adjustment methods. The downward adjustment found in our sample was minimal, with greatest identified attenuation of b = -.032, 95\% highest posterior density interval (HPD) ranging from -.055 to -.009, for the precision effect test (PET). Some methods tended to adjust upward, and this was especially true for data sets with a sample size smaller than 10. We propose that researchers should seek to explore the full range of plausible estimates for the effects they are studying and note that these methods may not be able to combat bias in small samples (with less than 10 primary studies). We argue that although the effect size attenuation we found tended to be minimal, this should not be taken as an indication of low levels of publication bias in psychology. We discuss the findings with reference to new developments in Bayesian methods for publication bias adjustment, and the recent methodological reforms in psychology. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english},
  pmid = {35446048},
  keywords = {Bayes Theorem,Bias,Humans,Publication Bias},
  file = {C:\Users\witol\Zotero\storage\HMEPERGI\Sladekova et al. - 2023 - Estimating the change in meta-analytic effect size estimates after the application of publication bi.pdf}
}

@article{vanzwetStatisticalPropertiesRCTs2021,
  title = {The Statistical Properties of {{RCTs}} and a Proposal for Shrinkage},
  author = {{van Zwet}, Erik and Schwab, Simon and Senn, Stephen},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6107--6117},
  issn = {1097-0258},
  doi = {10.1002/sim.9173},
  urldate = {2022-09-28},
  abstract = {We abstract the concept of a randomized controlled trial as a triple ({$\beta$},b,s), where {$\beta$} is the primary efficacy parameter, b the estimate, and s the standard error (s{$>$}0). If the parameter {$\beta$} is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z=b/s and the signal-to-noise ratio SNR={$\beta$}/s from a sample of pairs (bi,si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on ({$\beta$},b,s) only through the pair (z,SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of {$\beta$} is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  langid = {english},
  keywords = {achieved power,Cochrane review,exaggeration,randomized controlled trial,type M error},
  file = {C\:\\Users\\witol\\Zotero\\storage\\CMRW568V\\van Zwet et al. - 2021 - The statistical properties of RCTs and a proposal .pdf;C\:\\Users\\witol\\Zotero\\storage\\X8SIFSTA\\sim.html}
}
