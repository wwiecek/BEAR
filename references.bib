@article{addaPhackingClinicalTrials2020,
  title = {P-Hacking in Clinical Trials and How Incentives Shape the Distribution of Results across Phases},
  author = {Adda, J{\'e}r{\^o}me and Decker, Christian and Ottaviani, Marco},
  year = {2020},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {24},
  pages = {13386--13392},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1919906117},
  urldate = {2025-03-05},
  abstract = {Clinical research should conform to high standards of ethical and scientific integrity, given that human lives are at stake. However, economic incentives can generate conflicts of interest for investigators, who may be inclined to withhold unfavorable results or even tamper with data in order to achieve desired outcomes. To shed light on the integrity of clinical trial results, this paper systematically analyzes the distribution of P values of primary outcomes for phase II and phase III drug trials reported to the ClinicalTrials.gov registry. First, we detect no bunching of results just above the classical 5\% threshold for statistical significance. Second, a density-discontinuity test reveals an upward jump at the 5\% threshold for phase III results by small industry sponsors. Third, we document a larger fraction of significant results in phase III compared to phase II. Linking trials across phases, we find that early favorable results increase the likelihood of continuing into the next phase. Once we take into account this selective continuation, we can explain almost completely the excess of significant results in phase III for trials conducted by large industry sponsors. For small industry sponsors, instead, part of the excess remains unexplained.},
  file = {C:\Users\witol\Zotero\storage\H4SAHXGG\Adda et al. - 2020 - P-hacking in clinical trials and how incentives shape the distribution of results across phases.pdf}
}

@article{arel-bundockQuantitativePoliticalScience2022,
  title = {Quantitative {{Political Science Research}} Is {{Greatly Underpowered}}},
  author = {{Arel-Bundock}, Vincent and Briggs, Ryan C. and Doucouliagos, Hristos and Mendoza Avi{\~n}a, Marco and Stanley, Tom D.},
  year = {2022},
  journal = {I4R Discussion Paper Series},
  number = {6},
  publisher = {The Institute for Replication (I4R)},
  urldate = {2025-03-05},
  abstract = {The social sciences face a replicability crisis. A key determinant of replication success is statistical power. We assess the power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles. Using generous assumptions, we find that the median analysis has about 10\% power and that only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. We also find substantial heterogeneity in tests across research areas, with some being characterized by high power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  langid = {english},
  file = {C\:\\Users\\witol\\Zotero\\storage\\AVSH745R\\Arel-Bundock et al. - 2022 - Quantitative Political Science Research is Greatly Underpowered.pdf;C\:\\Users\\witol\\Zotero\\storage\\VYFBFQPW\\6.html}
}

@article{askarovSignificanceDataSharingPolicy2023,
  title = {The {{Significance}} of {{Data-Sharing Policy}}},
  author = {Askarov, Zohid and Doucouliagos, Anthony and Doucouliagos, Hristos and Stanley, T D},
  year = {2023},
  month = jun,
  journal = {Journal of the European Economic Association},
  volume = {21},
  number = {3},
  pages = {1191--1226},
  issn = {1542-4766},
  doi = {10.1093/jeea/jvac053},
  urldate = {2025-03-05},
  abstract = {We assess the impact of mandating data-sharing in economics journals on two dimensions of research credibility: statistical significance and excess statistical significance (ESS). ESS is a necessary condition for publication selection bias. Quasi-experimental difference-in-differences analysis of 20,121 estimates published in 24 general interest and leading field journals shows that data-sharing policies have reduced reported statistical significance and the associated t-values. The magnitude of this reduction is large and of practical significance. We also find suggestive evidence that mandatory data-sharing reduces ESS and hence decreases publication bias.},
  file = {C:\Users\witol\Zotero\storage\UW9X3C2I\6706852.html}
}

@article{barnettExaminationCIsHealth2019,
  title = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019: An Observational Study},
  shorttitle = {Examination of {{CIs}} in Health and Medical Journals from 1976 to 2019},
  author = {Barnett, Adrian Gerard and Wren, Jonathan D.},
  year = {2019},
  month = nov,
  journal = {BMJ open},
  volume = {9},
  number = {11},
  pages = {e032506},
  issn = {2044-6055},
  doi = {10.1136/bmjopen-2019-032506},
  abstract = {OBJECTIVES: Previous research has shown clear biases in the distribution of published p values, with an excess below the 0.05 threshold due to a combination of p-hacking and publication bias. We aimed to examine the bias for statistical significance using published confidence intervals. DESIGN: Observational study. SETTING: Papers published in Medline since 1976. PARTICIPANTS: Over 968\,000 confidence intervals extracted from abstracts and over 350\,000 intervals extracted from the full-text. OUTCOME MEASURES: Cumulative distributions of lower and upper confidence interval limits for ratio estimates. RESULTS: We found an excess of statistically significant results with a glut of lower intervals just above one and upper intervals just below 1. These excesses have not improved in recent years. The excesses did not appear in a set of over 100\,000 confidence intervals that were not subject to p-hacking or publication bias. CONCLUSIONS: The huge excesses of published confidence intervals that are just below the statistically significant threshold are not statistically plausible. Large improvements in research practice are needed to provide more results that better reflect the truth.},
  langid = {english},
  pmcid = {PMC6887056},
  pmid = {31753893},
  keywords = {Bibliometrics,confidence intervals,Confidence Intervals,Data Interpretation Statistical,MEDLINE,p-hacking,p-values,Probability,publication bias,Publication Bias,statistical significance},
  file = {C:\Users\witol\Zotero\storage\VJX3IMK5\Barnett and Wren - 2019 - Examination of CIs in health and medical journals from 1976 to 2019 an observational study.pdf}
}

@misc{brodeurPreRegistrationPreAnalysisPlans2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Do {{Pre-Registration}} and {{Pre-Analysis Plans Reduce}} p-{{Hacking}} and {{Publication Bias}}?},
  author = {Brodeur, Abel and Cook, Nikolai and Hartley, Jonathan and Heyes, Anthony},
  year = {2022},
  month = aug,
  number = {4180594},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4180594},
  urldate = {2022-09-07},
  abstract = {Randomized controlled trials (RCTs) are increasingly prominent in economics, with pre-registration and pre-analysis plans (PAPs) promoted as important in ensuring the credibility of findings. We investigate whether these tools reduce the extent of p-hacking and publication bias by collecting and studying the universe of test statistics, 15,992 in total, from RCTs published in 15 leading economics journals from 2018 through 2021. In our primary analysis, we find no meaningful difference in the distribution of test statistics from pre-registered studies, compared to their non-pre-registered counterparts. However, pre-registered studies that have a complete PAP are significantly less p-hacked. These results point to the importance of PAPs, rather than pre-registration in itself, in ensuring credibility.},
  langid = {english},
  keywords = {p-Hacking,Pre-analysis Plan,Pre-registration,Publication Bias,Research Credibility},
  file = {C\:\\Users\\witol\\Zotero\\storage\\PFDU4H8L\\Brodeur et al. - 2022 - Do Pre-Registration and Pre-Analysis Plans Reduce .pdf;C\:\\Users\\witol\\Zotero\\storage\\I8KYX62G\\papers.html}
}

@article{costelloDeclineEffectsAre2022,
  title = {Decline Effects Are Rare in Ecology},
  author = {Costello, Laura and Fox, Jeremy W.},
  year = {2022},
  journal = {Ecology},
  volume = {103},
  number = {6},
  pages = {e3680},
  issn = {1939-9170},
  doi = {10.1002/ecy.3680},
  urldate = {2025-03-05},
  abstract = {The scientific evidence base on any given topic changes over time as more studies are published. Currently, there is widespread concern about nonrandom, directional changes over time in the scientific evidence base associated with many topics. In particular, if studies finding large effects (e.g., large differences between treatment and control means) tend to get published quickly, while small effects tend to get published slowly, the net result will be a decrease over time in the estimated magnitude of the mean effect size, known as a ``decline effect.'' If decline effects are common, then the published scientific literature will provide a biased and misleading guide to management decisions, and to the allocation of future research effort. We compiled data from 466 meta-analyses in ecology to look for evidence of decline effects. We found that decline effects are rare. Only 5\% of ecological meta-analyses truly exhibit a directional change in mean effect size over time arising for some reason other than random chance, usually but not always in the direction of decline. Most apparent directional changes in mean effect size over time are attributable to regression to the mean, consistent with primary studies being published in random order with respect to the effect sizes they report. Our results are good news: decline effects are the exception to the rule in ecology. Identifying and rectifying rare cases of true decline effects remains an important task, but ecologists should not overgeneralize from anecdotal reports of decline effects.},
  copyright = {{\copyright} 2022 The Ecological Society of America.},
  langid = {english},
  keywords = {decline effect,effect size,hierarchical mixed effects model,meta-analysis,meta-meta-analysis,publication bias},
  file = {C:\Users\witol\Zotero\storage\KHLQDNXX\ecy.html}
}

@article{jagerEstimateSciencewiseFalse2014,
  title = {An Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature},
  author = {Jager, Leah R. and Leek, Jeffrey T.},
  year = {2014},
  month = jan,
  journal = {Biostatistics (Oxford, England)},
  volume = {15},
  number = {1},
  pages = {1--12},
  issn = {1468-4357},
  doi = {10.1093/biostatistics/kxt007},
  abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported \$P\$-values as the data. We then collect \$P\$-values from the abstracts of all 77\,430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported \$P\$-values. We estimate that the overall rate of false discoveries among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5\% more false positives (FP) per year, \$P = 0.18\$) or with respect to journal submissions (0.5\% more FP per 100 submissions, \$P = 0.12\$). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
  langid = {english},
  pmid = {24068246},
  keywords = {Algorithms,Biomedical Research,Computer Simulation,Data Interpretation Statistical,False discovery rate,False Positive Reactions,Genomics,Humans,Meta-analysis,Multiple testing,Publications,Science-wise false discovery rate,Software,Two-group model,United Kingdom,United States}
}

@misc{MetapsyMetaAnalyticPsychotherapy,
  title = {Metapsy {\textbar} {{Meta-Analytic Psychotherapy Databases}}},
  urldate = {2025-03-05},
  howpublished = {https://www.metapsy.org/},
  file = {C:\Users\witol\Zotero\storage\HPYCZSPJ\www.metapsy.org.html}
}

@article{sladekovaEstimatingChangeMetaanalytic2023,
  title = {Estimating the Change in Meta-Analytic Effect Size Estimates after the Application of Publication Bias Adjustment Methods},
  author = {Sladekova, Martina and Webb, Lois E. A. and Field, Andy P.},
  year = {2023},
  month = jun,
  journal = {Psychological Methods},
  volume = {28},
  number = {3},
  pages = {664--686},
  issn = {1939-1463},
  doi = {10.1037/met0000470},
  abstract = {Publication bias poses a challenge for accurately synthesizing research findings using meta-analysis. A number of statistical methods have been developed to combat this problem by adjusting the meta-analytic estimates. Previous studies tended to apply these methods without regard to optimal conditions for each method's performance. The present study sought to estimate the typical effect size attenuation of these methods when they are applied to real meta-analytic data sets that match the conditions under which each method is known to remain relatively unbiased (such as sample size, level of heterogeneity, population effect size, and the level of publication bias). Four-hundred and 33 data sets from 90 articles published in psychology journals were reanalyzed using a selection of publication bias adjustment methods. The downward adjustment found in our sample was minimal, with greatest identified attenuation of b = -.032, 95\% highest posterior density interval (HPD) ranging from -.055 to -.009, for the precision effect test (PET). Some methods tended to adjust upward, and this was especially true for data sets with a sample size smaller than 10. We propose that researchers should seek to explore the full range of plausible estimates for the effects they are studying and note that these methods may not be able to combat bias in small samples (with less than 10 primary studies). We argue that although the effect size attenuation we found tended to be minimal, this should not be taken as an indication of low levels of publication bias in psychology. We discuss the findings with reference to new developments in Bayesian methods for publication bias adjustment, and the recent methodological reforms in psychology. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english},
  pmid = {35446048},
  keywords = {Bayes Theorem,Bias,Humans,Publication Bias},
  file = {C:\Users\witol\Zotero\storage\HMEPERGI\Sladekova et al. - 2023 - Estimating the change in meta-analytic effect size estimates after the application of publication bi.pdf}
}

@article{vanzwetStatisticalPropertiesRCTs2021,
  title = {The Statistical Properties of {{RCTs}} and a Proposal for Shrinkage},
  author = {{van Zwet}, Erik and Schwab, Simon and Senn, Stephen},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6107--6117},
  issn = {1097-0258},
  doi = {10.1002/sim.9173},
  urldate = {2022-09-28},
  abstract = {We abstract the concept of a randomized controlled trial as a triple ({$\beta$},b,s), where {$\beta$} is the primary efficacy parameter, b the estimate, and s the standard error (s{$>$}0). If the parameter {$\beta$} is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z=b/s and the signal-to-noise ratio SNR={$\beta$}/s from a sample of pairs (bi,si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on ({$\beta$},b,s) only through the pair (z,SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of {$\beta$} is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  langid = {english},
  keywords = {achieved power,Cochrane review,exaggeration,randomized controlled trial,type M error},
  file = {C\:\\Users\\witol\\Zotero\\storage\\CMRW568V\\van Zwet et al. - 2021 - The statistical properties of RCTs and a proposal .pdf;C\:\\Users\\witol\\Zotero\\storage\\X8SIFSTA\\sim.html}
}

% POLITICS

@article{garisto2025trump,
  title={Trump order gives political appointees vast powers over research grants},
  author={Garisto, Dan},
  journal={Nature},
  volume={644},
  number={8077},
  pages={585--586},
  year={2025},
  publisher={Nature}
}

% BAD SCIENCE - paper mills

@article{aquarius2025tackling,
  title={Tackling paper mills requires us to prevent future contamination and clean up the past--the case of the journal Bioengineered},
  author={Aquarius, Ren{\'e} and Bik, Elisabeth M and Bimler, David and Oksvold, Morten P and Patrick, Kevin},
  journal={Bioengineered},
  volume={16},
  number={1},
  pages={2542668},
  year={2025},
  publisher={Taylor \& Francis}
}

@article{matusz2025threat,
  title={The threat of paper mills to biomedical and social science journals: The case of the {Tanu.pro} paper mill in \emph{Mind, Brain, and Education}},
  author={Matusz, Pawel J and Abalkina, Anna and Bishop, Dorothy V M},
  journal={Mind, Brain, and Education},
  volume={19},
  number={2},
  pages={90--100},
  year={2025},
  publisher={Wiley Online Library}
}


% COMMENTARIES

@article{benjaminRedefineStatisticalSignificance2018a,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2018},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {1},
  pages = {6--10},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  urldate = {2025-08-18},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Statistics}
}

@article{ioannidisDiscussionWhyEstimate2014,
  title = {Discussion: {{Why}} “{{An}} Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature” Is False},
  shorttitle = {Discussion},
  author = {Ioannidis, John P. A.},
  date = {2014-01-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {15},
  number = {1},
  pages = {28--36},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxt036},
  url = {https://doi.org/10.1093/biostatistics/kxt036},
  urldate = {2025-09-02}
}

@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John P A},
  journal={PLoS Medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}

@article{buttonPowerFailureWhy2013,
  title = {Power failure: Why small sample size undermines the reliability of n euroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2025-08-18},
  copyright = {2013 Springer Nature Limited},
  langid = {english},
  keywords = {Molecular neuroscience}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015a,
  title = {Estimating the reproducibility of psychological science},
  author = {{Open Science Collaboration}},
  year = {2015},
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
  author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
  year = {2011},
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366}
}

@article{camerer2016evaluating,
  title={Evaluating replicability of laboratory experiments in economics},
  author={Camerer, Colin F and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and others},
  journal={Science},
  volume={351},
  number={6280},
  pages={1433--1436},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@article{nosek2020replication,
  title={What is replication?},
  author={Nosek, Brian A and Errington, Timothy M},
  journal={PLoS Biology},
  volume={18},
  number={3},
  pages={e3000691},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {ASA} statement on p-values: Context, Process and Purpose},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  journal = {American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133}
}


@article{bak2022revisiting,
  title={Revisiting the replication crisis without false positives},
  author={Bak-Coleman, Joseph and Mann, Richard and Bergstrom, Carl and Gross, Kevin and West, Jevin},
  year={2022},
  journal={Center for Open Science},
note={\url{https://osf.io/preprints/socarxiv/rkyf7_v1}}
}

@article{neves2022most,
  title={Are most published research findings false in a continuous universe?},
  author={Neves, Kleber and Tan, Pedro B and Amaral, Olavo B},
  journal={Plos One},
  volume={17},
  number={12},
  pages={e0277935},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}


%
% PUBLICATIONS
%

@article{schwab2020,
    title = {Re-estimating 400,000 treatment effects from intervention studies in the Cochrane database of systematic reviews [Data set]},
    author = {Simon Schwab},
    journal = {Open Science Framework},
    year = {2020},
    note = {\url{https://doi.org/10.17605/OSF.IO/XJV9G}}
  }


@article{askarov2023significance,
  title={The significance of data-sharing policy},
  author={Askarov, Zohid and Doucouliagos, Anthony and Doucouliagos, Hristos and Stanley, T D},
  journal={Journal of the European Economic Association},
  volume={21},
  number={3},
  pages={1191--1226},
  year={2023},
  publisher={Oxford University Press}
}

@article{brodeur2024preregistration,
  title={Do preregistration and preanalysis plans reduce p-hacking and publication bias? Evidence from 15,992 test statistics and suggestions for improvement},
  author={Brodeur, Abel and Cook, Nikolai M and Hartley, Jonathan S and Heyes, Anthony},
  journal={Journal of Political Economy Microeconomics},
  volume={2},
  number={3},
  pages={527--561},
  year={2024},
  publisher={The University of Chicago Press Chicago, IL}
}

@article{arel2022quantitative,
  title={Quantitative political science research is greatly underpowered},
  author={Arel-Bundock, Vincent and Briggs, Ryan C and Doucouliagos, Hristos and Mendoza Aviña, Marco and Stanley, Tom D},
  year={2026},
  journal={Journal of Politics}
}

@article{jager2014estimate,
  title={An estimate of the science-wise false discovery rate and application to the top medical literature},
  author={Jager, Leah R and Leek, Jeffrey T},
  journal={Biostatistics},
  volume={15},
  number={1},
  pages={1--12},
  year={2014},
  publisher={Oxford University Press}
}

@article{adda2020p,
  title={P-hacking in clinical trials and how incentives shape the distribution of results across phases},
  author={Adda, J{\'e}r{\^o}me and Decker, Christian and Ottaviani, Marco},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={24},
  pages={13386--13392},
  year={2020},
  publisher={National Acad Sciences}
}

@article{barnett2019examination,
  title={Examination of {CIs} in health and medical journals from 1976 to 2019: an observational study},
  author={Barnett, Adrian Gerard and Wren, Jonathan D},
  journal={BMJ Open},
  volume={9},
  number={11},
  pages={e032506},
  year={2019},
  publisher={British Medical Journal Publishing Group}
}


@article{sladekova2023estimating,
  title={Estimating the change in meta-analytic effect size estimates after the application of publication bias adjustment methods.},
  author={Sladekova, Martina and Webb, Lois E A and Field, Andy P},
  journal={Psychological Methods},
  volume={28},
  number={3},
  pages={664},
  year={2023},
  publisher={American Psychological Association}
}

@article{bartovs2023meta,
  title={Meta-analyses in psychology often overestimate evidence for and size of effects},
  author={Barto{\v{s}}, Franti{\v{s}}ek and Maier, Maximilian and Shanks, David R and Stanley, T D and Sladekova, Martina and Wagenmakers, Eric-Jan},
  journal={Royal Society Open Science},
  volume={10},
  number={7},
  pages={230224},
  year={2023},
  publisher={The Royal Society}
}

@article{cuijpers2024effects,
title={Cognitive behavior therapy for mental disorders in adults: A unified series of meta-analyses},
author={Cuijpers, Pim and Harrer, Mathias and Miguel, Clara and others},
  year={2025},
  journal={JAMA Psychiatry},
volume={82},
number={6},
pages={563--571}
}

@article{costello2022decline,
  title={Decline effects are rare in ecology},
  author={Costello, Laura and Fox, Jeremy W},
  journal={Ecology},
  volume={103},
  number={6},
  pages={e3680},
  year={2022},
  publisher={Wiley Online Library}
}

@article{yang2023publication,
  title={Publication bias impacts on effect size, statistical power, and magnitude (type {M}) and sign (type {S}) errors in ecology and evolutionary biology},
  author={Yang, Yefeng and S{\'a}nchez-T{\'o}jar, Alfredo and O’Dea, Rose E and Noble, Daniel W A and Koricheva, Julia and Jennions, Michael D and Parker, Timothy H and Lagisz, Malgorzata and Nakagawa, Shinichi},
  journal={BMC Biology},
  volume={21},
  number={1},
  pages={71},
  year={2023},
  publisher={Springer}
}

@article{yang2024large,
  title={A large-scale in silico replication of ecological and evolutionary studies},
  author={Yang, Yefeng and Zwet, Erik van and Ignatiadis, Nikolaos and Nakagawa, Shinichi},
  journal={Nature Ecology \& Evolution},
  pages={1--5},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{chavalarias2016evolution,
  title={Evolution of reporting P values in the biomedical literature, 1990-2015},
  author={Chavalarias, David and Wallach, Joshua David and Li, Alvin Ho Ting and Ioannidis, John P A},
  journal={Journal of the American Medical Association},
  volume={315},
  number={11},
  pages={1141--1148},
  year={2016},
  publisher={American Medical Association}
}

@article{head2015extent,
  title={The extent and consequences of p-hacking in science},
  author={Head, Megan L and Holman, Luke and Lanfear, Rob and Kahn, Andrew T and Jennions, Michael D},
  journal={PLoS Biology},
  volume={13},
  number={3},
  pages={e1002106},
  year={2015},
  publisher={Public Library of Science}
}

% THEORY

@article{van2021statistical,
  title={The statistical properties of RCTs and a proposal for shrinkage},
  author={{van Zwet}, Erik and Schwab, Simon and Senn, Stephen},
  journal={Statistics in Medicine},
  volume={40},
  number={27},
  pages={6107--6117},
  year={2021},
  publisher={Wiley Online Library}
}

@article{zwet2022proposal,
  title={A proposal for informative default priors scaled by the standard error of estimates},
  author={{van Zwet}, Erik and Gelman, Andrew},
  journal={American Statistician},
  volume={76},
  number={1},
  pages={1--9},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{efron2016empirical,
  title={Empirical Bayes deconvolution estimates},
  author={Efron, Bradley},
  journal={Biometrika},
  volume={103},
  number={1},
  pages={1--20},
  year={2016},
  publisher={Oxford University Press}
}

@article{stephens2017false,
  title={False discovery rates: a new deal},
  author={Stephens, Matthew},
  journal={Biostatistics},
  volume={18},
  number={2},
  pages={275--294},
  year={2017},
  publisher={Oxford University Press}
}

@article{hedges1984estimation,
  title={Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences},
  author={Hedges, Larry V},
  journal={Journal of Educational Statistics},
  volume={9},
  number={1},
  pages={61--85},
  year={1984},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{hedges1992modeling,
  title={Modeling publication selection effects in meta-analysis},
  author={Hedges, Larry V},
  journal={Statistical Science},
  volume={7},
  number={2},
  pages={246--255},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}

@article{cox2014discussion,
  title={Discussion: Comment on a paper by Jager and Leek},
  author={Cox, David R},
  journal={Biostatistics},
  volume={15},
  number={1},
  pages={16--18},
  year={2014},
  publisher={Oxford University Press}
}



@article{van2021significance,
  title={The significance filter, the winner's curse and the need to shrink},
  author={{van Zwet}, Erik and Cator, Eric A},
  journal={Statistica Neerlandica},
  volume={75},
  number={4},
  pages={437--452},
  year={2021},
  publisher={Wiley Online Library}
}

@article{single_meta,
  title={Meta-analysis with a single study},
  author={{van Zwet}, Erik and Wi\k{e}cek, Witold and Gelman, Andrew},
  year={2025},
  journal={Statistical Methods in Medical Research}
}

@article{rufibach2016bayesian,
  title={Bayesian predictive power: choice of prior and some recommendations for its use as probability of success in drug development},
  author={Rufibach, Kaspar and Burger, Hans Ulrich and Abt, Markus},
  journal={Pharmaceutical statistics},
  volume={15},
  number={5},
  pages={438--446},
  year={2016},
  publisher={Wiley Online Library}
}

@article{gelman2013commentary,
  title={Commentary: P Values and Statistical Practice},
  author={Gelman, Andrew},
  journal={Epidemiology},
  volume={24},
  number={1},
  pages={69--72},
  year={2013},
  publisher={LWW}
}

@article{gelman2020holes,
  title={Holes in Bayesian statistics},
  author={Gelman, Andrew and Yao, Yuling},
  journal={Journal of Physics G: Nuclear and Particle Physics},
  volume={48},
  number={1},
  pages={014002},
  year={2020},
  publisher={IOP Publishing}
}

@article{gelman2006difference,
  title={The difference between ``significant'' and ``not significant'' is not itself statistically significant},
  author={Gelman, Andrew and Stern, Hal},
  journal={American Statistician},
  volume={60},
  number={4},
  pages={328--331},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{greenland2013living,
  title={Living with p values: resurrecting a {Bayesian} perspective on frequentist statistics},
  author={Greenland, Sander and Poole, Charles},
  journal={Epidemiology},
  volume={24},
  number={1},
  pages={62--68},
  year={2013},
  publisher={LWW}
}

@article{wagenmakers2023history,
  title={History and nature of the Jeffreys--Lindley paradox},
  author={Wagenmakers, Eric-Jan and Ly, Alexander},
  journal={Archive for History of Exact Sciences},
  volume={77},
  number={1},
  pages={25--72},
  year={2023},
  publisher={Springer}
}

@article{goodman1992comment,
  title={A comment on replication, p-values and evidence},
  author={Goodman, Steven N},
  journal={Statistics in medicine},
  volume={11},
  number={7},
  pages={875--879},
  year={1992},
  publisher={Wiley Online Library}
}

@article{vanzwet2022large,
  title={How large should the next study be? Predictive power and sample size requirements for replication studies},
  author={{van Zwet}, Erik and Goodman, Steven N},
  journal={Statistics in Medicine},
  volume={41},
  number={16},
  pages={3090--3101},
  year={2022},
  publisher={Wiley Online Library}
}

@article{senn2002comment,
  title={A comment on replication, p-values and evidence SN Goodman, Statistics in Medicine 1992; 11: 875-879},
  author={Senn, Stephen},
  journal={Statistics in medicine},
  volume={21},
  number={16},
  pages={2437--2444},
  year={2002},
  publisher={Wiley Online Library}
}

@article{open2015estimating,
  title={Estimating the reproducibility of psychological science},
  author={{Open Science Collaboration}},
  journal={Science},
  volume={349},
  number={6251},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{greenland2017invited,
  title={The need for cognitive science in methodology},
  author={Greenland, Sander},
  journal={American Journal of Epidemiology},
  volume={186},
  number={6},
  pages={639--645},
  year={2017},
  publisher={Oxford University Press}
}

@article{gelman2014beyond,
  title={Beyond power calculations: Assessing type {S} (sign) and type {M} (magnitude) errors},
  author={Gelman, Andrew and Carlin, John},
  journal={Perspectives on Psychological Science},
  volume={9},
  number={6},
  pages={641--651},
  year={2014},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{gelman2000type,
  title={Type S error rates for classical and Bayesian single and multiple comparison procedures},
  author={Gelman, Andrew and Tuerlinckx, Francis},
  journal={Computational Statistics},
  volume={15},
  number={3},
  pages={373--390},
  year={2000},
  publisher={Springer}
}

@article{tversky1971,
author={Tversky, Amos and Kahneman, Daniel},
year={1971},
title={Belief in the law of small numbers},
journal={Psychological Bulletin},
volume={76},
pages={105--110}
}
